WEBVTT

00:00.000 --> 00:05.680
Hi everyone. So by now you have probably heard of chatGPT. It has taken the world and AI community

00:05.680 --> 00:12.560
by storm and it is a system that allows you to interact with an AI and give it text-based tasks.

00:12.560 --> 00:16.960
So for example, we can ask chatGPT to write us a small haiku about how important it is that

00:16.960 --> 00:20.720
people understand AI and then they can use it to improve the world and make it more prosperous.

00:21.360 --> 00:27.040
So when we run this, AI knowledge brings prosperity for all to see embrace its power.

00:27.040 --> 00:33.280
Okay, not bad. And so you could see that chatGPT went from left to right and generated all these

00:33.280 --> 00:39.840
words sequentially. Now I asked it already the exact same prompt a little bit earlier and it

00:39.840 --> 00:45.440
generated a slightly different outcome. AI's power to grow, ignorance holds us back, learn,

00:45.440 --> 00:51.280
prosperity waits. So pretty good in both cases and slightly different. So you can see that chatGPT

00:51.280 --> 00:56.960
is a probabilistic system and for anyone prompt it can give us multiple answers, sort of replying

00:56.960 --> 01:02.400
to it. Now this is just one example of a prompt. People have come up with many, many examples

01:02.400 --> 01:08.240
and there are entire websites that index interactions with chatGPT. And so many of them

01:08.240 --> 01:13.520
are quite humorous. Explain HTML to me like I'm a dog. Write release notes for chess too.

01:14.480 --> 01:21.280
Write a note about Elon Musk buying a Twitter and so on. So as an example, please write a breaking

01:21.280 --> 01:27.280
news article about a leaf falling from a tree and a shocking turn of events. A leaf has fallen from

01:27.280 --> 01:31.600
a tree in the local park. Witnesses report that the leaf, which was previously attached to a branch

01:31.600 --> 01:37.360
of a tree, detached itself and fell to the ground. Very dramatic. So you can see that this is a pretty

01:37.360 --> 01:45.280
remarkable system and it is what we call a language model because it models the sequence of words or

01:45.280 --> 01:50.960
characters or tokens more generally and it knows how sort of words follow each other in English

01:50.960 --> 01:57.520
language. And so from its perspective, what it is doing is it is completing the sequence. So I give

01:57.520 --> 02:02.800
it the start of a sequence and it completes the sequence with the outcome. And so it's a language

02:02.800 --> 02:09.520
model in that sense. Now I would like to focus on the under the hood of under the hood components of

02:09.520 --> 02:14.800
what makes chatGPT work. So what is the neural network under the hood that models the sequence of

02:14.800 --> 02:22.160
these words? And that comes from this paper called attention is all you need. In 2017, a landmark

02:22.160 --> 02:29.520
paper, landmark paper in AI that produced and proposed the transformer architecture. So GPT

02:29.520 --> 02:36.160
is short for generatively pre-trained transformer. So transformer is the neural net that actually

02:36.160 --> 02:41.760
does all the heavy lifting under the hood. It comes from this paper in 2017. Now if you read this

02:41.760 --> 02:46.560
paper, this reads like a pretty random machine translation paper. And that's because I think

02:46.560 --> 02:50.240
the authors didn't fully anticipate the impact that the transformer would have on the field.

02:50.880 --> 02:54.560
And this architecture that they produced in the context of machine translation,

02:54.560 --> 03:01.360
in their case, actually ended up taking over the rest of AI in the next five years after. And so

03:01.360 --> 03:07.360
this architecture with minor changes was copy pasted into a huge amount of applications in AI

03:07.360 --> 03:14.560
in more recent years. And that includes at the core of chatGPT. Now we are not going to, what I'd

03:14.560 --> 03:19.680
like to do now is I'd like to build out something like chatGPT. But we're not going to be able to,

03:19.680 --> 03:24.560
of course, reproduce chatGPT. This is a very serious production grade system. It is trained on

03:25.920 --> 03:31.520
a good chunk of internet. And then there's a lot of pre-training and fine tuning stages to it.

03:31.520 --> 03:37.360
And so it's very complicated. What I'd like to focus on is just to train a transformer based

03:37.360 --> 03:42.800
language model. And in our case, it's going to be a character level language model. I still think

03:42.800 --> 03:47.600
that is a very educational with respect to how these systems work. So I don't want to train on

03:47.600 --> 03:52.240
the chunk of internet. We need a smaller data set. In this case, I propose that we work with

03:52.800 --> 03:57.840
my favorite toy data set. It's called Tiny Shakespeare. And what it is is basically it's

03:57.840 --> 04:03.360
a concatenation of all of the works of Shakespeare in my understanding. And so this is all of Shakespeare

04:03.360 --> 04:08.480
in a single file. This file is about one megabyte. And it's just all of Shakespeare.

04:09.520 --> 04:13.920
And what we are going to do now is we're going to basically model how these characters

04:13.920 --> 04:17.840
follow each other. So for example, given a chunk of these characters like this,

04:18.880 --> 04:24.480
given some context of characters in the past, the transformer neural network will look at

04:24.480 --> 04:28.960
the characters that I've highlighted and is going to predict that G is likely to come next in the

04:28.960 --> 04:34.000
sequence. And it's going to do that because we're going to train that transformer on Shakespeare.

04:34.000 --> 04:40.240
And it's just going to try to produce character sequences that look like this. And in that process

04:40.240 --> 04:45.200
is going to model all the patterns inside this data. So once we've trained the system, I just

04:45.200 --> 04:50.400
like to give you a preview, we can generate infinite Shakespeare. And of course, it's a fake

04:50.400 --> 05:00.080
thing that looks kind of like Shakespeare. Apologies for there's some jank that I'm not able to resolve

05:00.080 --> 05:07.120
in in here, but you can see how this is going character by character. And it's kind of like

05:07.120 --> 05:14.400
predicting Shakespeare like language. So verily, my Lord, the sites have left the again, the king

05:14.400 --> 05:21.440
coming with my curses with precious pale, and then traneosa something else, etc. And this is just

05:21.440 --> 05:26.800
coming out of the transformer in a very similar manner as it would come out in chat GPT. In our

05:26.800 --> 05:33.360
case character by character in chat GPT, it's coming out on the token by token level and tokens are

05:33.360 --> 05:38.160
these sort of like little subword pieces. So they're not word level, they're kind of like work chunk

05:38.160 --> 05:48.240
level. And now I've already written this entire code to train these transformers. And it is in

05:48.240 --> 05:54.160
a GitHub repository that you can find. And it's called nano GPT. So nano GPT is a repository

05:54.160 --> 06:00.400
that you can find on my GitHub. And it's a repository for training transformers on any given text.

06:01.040 --> 06:04.640
And what I think is interesting about it, because there's many ways to train transformers,

06:04.640 --> 06:09.920
but this is a very simple implementation. So it's just two files of 300 lines of code each,

06:09.920 --> 06:15.600
one file defies the GPT model, the transformer, and one file trains it on some given text data set.

06:16.320 --> 06:20.400
And here I'm showing that if you train it on a open web text data set, which is a fairly large

06:20.400 --> 06:27.760
data set of web pages, then I reproduce the performance of GPT two. So GPT two is an early

06:27.760 --> 06:35.440
version of OpenAI's GPT from 2017 if I recall correctly. And I've only so far reproduced the

06:35.440 --> 06:39.840
smallest 124 million parameter model. But basically, this is just proving that the code base is

06:39.840 --> 06:46.080
correctly arranged. And I'm able to load the neural network weights that OpenAI has released later.

06:46.960 --> 06:51.840
So you can take a look at the finished code here in nano GPT. But what I would like to do in this

06:51.840 --> 06:57.840
lecture is I would like to basically write this repository from scratch. So we're going to begin

06:57.840 --> 07:04.000
with an empty file. And we're going to define a transformer piece by piece. We're going to train

07:04.000 --> 07:09.520
it on the tiny Shakespeare data set. And we'll see how we can then generate infinite Shakespeare.

07:10.160 --> 07:15.680
And of course, this can copy paste to any arbitrary text data set that you like. But my goal really

07:15.680 --> 07:22.640
here is to just make you understand and appreciate how under the hood chat GPT works. And really,

07:22.640 --> 07:30.080
all that's required is a proficiency in Python and some basic understanding of calculus and statistics.

07:30.080 --> 07:35.200
And it would help if you also see my previous videos on the same YouTube channel, in particular,

07:35.200 --> 07:43.280
my Make More series, where I define smaller and simpler neural network language models. So multi

07:43.280 --> 07:48.800
parallel perceptrons and so on. It really introduces the language modeling framework. And then here

07:48.800 --> 07:53.440
in this video, we're going to focus on the transformer neural network itself. Okay, so I created

07:53.440 --> 07:59.280
a new Google collab Jupyter notebook here. And this will allow me to later easily share this

07:59.280 --> 08:03.280
code that we're going to develop together with you so you can follow along. So this will be in a

08:03.280 --> 08:09.600
video description later. Now, here, I've just done some preliminaries. I downloaded the data set,

08:09.600 --> 08:13.680
the tiny Shakespeare data set at this URL. And you can see that it's about a one megabyte file.

08:14.640 --> 08:18.560
Then here I opened the input at txt file and just reading all the text at a string.

08:19.360 --> 08:24.720
And we see that we are working with one million characters roughly. And the first 1000 characters,

08:24.720 --> 08:29.760
if we just print them out, are basically what you would expect. This is the first 1000 characters

08:29.760 --> 08:36.080
of the tiny Shakespeare data set, roughly up to here. So so far so good. Next, we're going to take

08:36.080 --> 08:42.000
this text. And the text is a sequence of characters in Python. So when I call the set constructor on

08:42.000 --> 08:48.640
it, I'm just going to get the set of all the characters that occur in this text. And then I

08:48.640 --> 08:53.520
call list on that to create a list of those characters instead of just a set so that I have

08:53.520 --> 08:59.920
an ordering an arbitrary ordering. And then I sort that. So basically, we get just all the characters

08:59.920 --> 09:04.320
that occur in the entire data set and they're sorted. Now the number of them is going to be our

09:04.320 --> 09:10.160
vocabulary size. These are the possible elements of our sequences. And we see that when I print here

09:10.160 --> 09:16.400
the characters, there's 65 of them in total. There's a space character, and then all kinds of

09:16.400 --> 09:22.720
special characters. And then capitals and lowercase letters. So that's our vocabulary. And that's the

09:22.720 --> 09:29.920
sort of like possible characters that the model can see or emit. Okay, so next we would like to

09:29.920 --> 09:36.800
develop some strategy to tokenize the input text. Now, when people say tokenize, they mean convert

09:36.800 --> 09:42.720
the raw text as a string to some sequence of integers according to some note, according to some

09:42.720 --> 09:48.400
vocabulary of possible elements. So as an example, here, we are going to be building a character

09:48.400 --> 09:52.640
level language model. So we're simply going to be translating individual characters into integers.

09:53.360 --> 09:58.640
So let me show you a chunk of code that sort of does that for us. So we're building both the encoder

09:58.640 --> 10:04.800
and the decoder. And let me just talk through what's happening here. When we encode an arbitrary

10:04.800 --> 10:11.680
text, like Hi, there, we're going to receive a list of integers that represents that string. So for

10:11.680 --> 10:19.040
example, 46 47, etc. And then we also have the reverse mapping. So we can take this list and

10:19.040 --> 10:24.640
decode it to get back the exact same string. So it's really just like a translation to integers

10:24.640 --> 10:30.720
and back for arbitrary string. And for us, it is done on a character level. Now the way this was

10:30.720 --> 10:35.840
achieved is we just iterate over all the characters here, and create a lookup table from the character

10:35.840 --> 10:41.600
to the integer and vice versa. And then to encode some string, we simply translate all the characters

10:41.600 --> 10:46.720
individually. And to decode it back, we use the reverse mapping, go and coordinate all of it.

10:47.440 --> 10:52.960
Now, this is only one of many possible encodings, or many possible sort of tokenizers, and it's a

10:52.960 --> 10:57.680
very simple one. But there's many other schemas that people have come up with in practice. So for

10:57.680 --> 11:05.280
example, Google uses sentence piece. So sentence piece will also encode text into integers, but

11:05.280 --> 11:11.840
in a different schema, and using a different vocabulary. And sentence piece is a subword

11:11.840 --> 11:17.200
sort of tokenizer. And what that means is that you're not encoding entire words, but you're not

11:17.200 --> 11:22.960
also encoding individual characters. It's a subword unit level. And that's usually what's

11:22.960 --> 11:28.320
adopted in practice. For example, also OpenAI has this library called tick token that uses a

11:28.320 --> 11:35.600
bipair encoding tokenizer. And that's what GPT uses. And you can also just encode words into

11:35.600 --> 11:41.360
like hell world into lists of integers. So as an example, I'm using the tick token library here.

11:42.080 --> 11:45.440
I'm getting the encoding for GPT2, or that was used for GPT2.

11:45.440 --> 11:53.360
Instead of just having 65 possible characters, or tokens, they have 50,000 tokens. And so when

11:53.360 --> 11:59.680
they encode the exact same string, hi there, we only get a list of three integers. But those integers

11:59.680 --> 12:09.120
are not between zero and 64. They are between zero and 50,256. So basically, you can trade off the

12:09.120 --> 12:14.800
codebook size and the sequence lengths. So you can have very long sequences of integers with very

12:14.800 --> 12:23.600
small vocabularies, or you can have short sequences of integers with very large vocabularies. And so

12:24.240 --> 12:29.600
typically people use in practice these subword encodings. But I'd like to keep our tokenizer

12:29.600 --> 12:33.760
very simple. So we're using character level tokenizer. And that means that we have very

12:33.760 --> 12:40.720
small codebooks. We have very simple encode and decode functions. But we do get very long sequences

12:40.720 --> 12:44.480
as a result. But that's the level at which we're going to stick with this lecture, because it's

12:44.480 --> 12:50.000
the simplest thing. Okay, so now that we have an encoder and a decoder, effectively a tokenizer,

12:50.000 --> 12:54.320
we can tokenize the entire training set of Shakespeare. So here's a chunk of code that

12:54.320 --> 12:59.200
does that. And I'm going to start to use the PyTorch library, and specifically the Torch.tensor

12:59.200 --> 13:05.040
from the PyTorch library. So we're going to take all of the text in tiny Shakespeare, encode it,

13:05.040 --> 13:10.480
and then wrap it into a Torch.tensor to get the data tensor. So here's what the data tensor looks

13:10.480 --> 13:16.320
like when I look at just the first 1000 characters, or the 1000 elements of it. So we see that we have

13:16.320 --> 13:21.280
a massive sequence of integers. And this sequence of integers here is basically an identical

13:21.280 --> 13:27.520
translation of the first 1000 characters here. So I believe, for example, that zero is a new line

13:27.520 --> 13:34.000
character. And maybe one is a space, not 100% sure. But from now on, the entire data set of text is

13:34.000 --> 13:40.480
rerepresented as just, it's just stretched out as a single very large sequence of integers. Let me

13:40.480 --> 13:45.600
do one more thing before we move on here. I'd like to separate out our data set into a train and a

13:45.600 --> 13:52.000
validation split. So in particular, we're going to take the first 90% of the data set and consider

13:52.000 --> 13:56.720
that to be the training data for the transformer. And we're going to withhold the last 10% at the end

13:56.720 --> 14:01.600
of it to be the validation data. And this will help us understand to what extent our model is

14:01.600 --> 14:06.640
overfitting. So we're going to basically hide and keep the validation data on the side, because we

14:06.640 --> 14:11.760
don't want just a perfect memorization of this exact Shakespeare, we want a neural network that

14:11.760 --> 14:17.040
sort of creates Shakespeare like text. And so it should be fairly likely for it to produce

14:17.920 --> 14:26.400
the actual like stowed away, true Shakespeare text. And so we're going to use this to get a sense of

14:26.400 --> 14:31.360
the overfitting. Okay, so now we would like to start plugging these text sequences or integer

14:31.360 --> 14:36.560
sequences into the transformer so that it can train and learn those patterns. Now, the important

14:36.560 --> 14:41.680
thing to realize is we're never going to actually feed entire text into transformer all at once,

14:41.680 --> 14:46.240
that would be computationally very expressive and prohibitive. So when we actually train a

14:46.240 --> 14:50.880
transformer on a lot of these data sets, we only work with chunks of the data set. And when we train

14:50.880 --> 14:56.000
the transformer, we basically sample random little chunks out of the training set and train on just

14:56.000 --> 15:02.640
chunks at a time. And these chunks have basically some kind of a length, and some maximum length.

15:03.440 --> 15:07.600
Now, the maximum length typically, at least in the code I usually write is called block size.

15:08.400 --> 15:13.280
You can you can find it on the different names like context length or something like that.

15:13.280 --> 15:17.840
Let's start with the block size of just eight. And let me look at the first train data characters,

15:18.480 --> 15:22.320
the first block size plus one characters, I'll explain why plus one in a second.

15:22.320 --> 15:28.640
So this is the first nine characters in the sequence in the training set. Now, what I'd like to point

15:28.640 --> 15:33.760
out is that when you sample a chunk of data like this, so set if these nine characters out of the

15:33.760 --> 15:40.240
training set, this actually has multiple examples packed into it. And that's because all of these

15:40.240 --> 15:46.640
characters follow each other. And so what this thing is going to say when we plug it into a

15:46.640 --> 15:51.200
transformer is we're going to actually simultaneously train it to make prediction at every one

15:51.200 --> 15:57.840
of these positions. Now, in the in a chunk of nine characters, there's actually eight individual

15:57.840 --> 16:05.600
examples packed in there. So there's the example that when 18, when in the context of 1847,

16:05.600 --> 16:14.480
likely comes next. In the context of 1847, 56 comes next. In the context of 1847, 56, 57 can come

16:14.480 --> 16:21.200
next, and so on. So that's the eight individual examples. Let me actually spell it out with code.

16:22.560 --> 16:27.600
So here's a chunk of code to illustrate. X are the inputs to the transformer. It will just be

16:27.600 --> 16:35.280
the first block size characters. Y will be the next block size characters. So it's offset by one.

16:36.000 --> 16:42.320
And that's because why are the targets for each position in the input. And then here I'm iterating

16:42.320 --> 16:49.120
over all the block size of eight. And the context is always all the characters in X up to T and

16:49.120 --> 16:56.320
including T. And the target is always the teeth character, but in the targets array Y. So let me

16:56.320 --> 17:02.640
just run this. And basically, it spells out what I said in words. These are the eight examples

17:02.640 --> 17:10.480
hidden in a chunk of nine characters that we sampled from the training set. I want to mention

17:10.480 --> 17:17.040
one more thing. We train on all the eight examples here with context between one all the way up to

17:17.040 --> 17:21.760
context of block size. And we train on that not just for computational reasons, because we happen

17:21.760 --> 17:26.400
to have the sequence already or something like that. It's not just done for efficiency. It's also

17:26.400 --> 17:33.520
done to make the transformer network be used to seeing contexts all the way from as little as one,

17:33.520 --> 17:38.480
all the way to block size. And we'd like the transformer to be used to seeing everything

17:38.480 --> 17:43.200
in between. And that's going to be useful later during inference, because while we're sampling,

17:43.200 --> 17:47.440
we can start to set a sampling generation with as little as one character of context.

17:47.440 --> 17:51.680
And then transformer knows how to predict next character with all the way up to just context

17:51.680 --> 17:56.560
of one. And so then it can predict everything up to block size. And after block size, we have to

17:56.560 --> 18:02.320
start truncating, because the transformer will never receive more than block size inputs when

18:02.320 --> 18:07.680
it's predicting the next character. Okay, so we've looked at the time dimension of the tensors

18:07.680 --> 18:11.120
that are going to be feeding into the transformer. There's one more dimension to care about, and that

18:11.120 --> 18:17.040
is the batch dimension. And so as we're sampling these chunks of text, we're going to be actually,

18:17.040 --> 18:20.560
every time we're going to feed them into a transformer, we're going to have many batches

18:20.560 --> 18:25.280
of multiple chunks of text that are all stacked up in a single tensor. And that's just done for

18:25.280 --> 18:30.880
efficiency, just so that we can keep the GPUs busy, because they are very good at parallel processing

18:30.880 --> 18:37.600
of data. And so we just want to process multiple chunks all at the same time. But those chunks

18:37.600 --> 18:41.760
are processed completely independently, they don't talk to each other, and so on. So let me

18:41.760 --> 18:45.840
basically just generalize this and introduce a batch dimension. Here's a chunk of code.

18:47.120 --> 18:49.440
Let me just run it, and then I'm going to explain what it does.

18:51.680 --> 18:55.760
So here, because we're going to start sampling random locations in the data sets to pull chunks

18:55.760 --> 19:02.240
from, I am setting the seed so that in the random number generator, so that the numbers I see here

19:02.240 --> 19:07.040
are going to be the same numbers you see later, if you try to reproduce this. Now the batch size

19:07.040 --> 19:11.680
here is how many independent sequences we are processing every forward backward pass of the

19:11.680 --> 19:17.600
transformer. The block size, as I explained, is the maximum context length to make those predictions.

19:18.240 --> 19:23.680
So let's say size four, block size eight, and then here's how we get batch for any arbitrary

19:23.680 --> 19:28.320
split. If the split is a training split, then we're going to look at train data, otherwise at val

19:28.320 --> 19:36.480
data, that gives us the data array. And then when I generate random positions to grab a chunk out of,

19:37.200 --> 19:44.640
I actually grab, I actually generate batch size number of random offsets. So because this is four,

19:44.640 --> 19:50.880
we are, ix is going to be a four numbers that are randomly generated between zero and length of data

19:50.880 --> 19:57.680
minus block size. So it's just random offsets into the training set. And then x is, as I explained,

19:57.680 --> 20:06.000
are the first block size characters starting at i. The y's are the offset by one of that,

20:06.000 --> 20:12.880
so just add plus one. And then we're going to get those chunks for every one of integers i in ix,

20:13.440 --> 20:20.640
and use a torch.stack to take all those one dimensional tensors as we saw here. And we're

20:20.640 --> 20:29.840
going to stack them up as rows. And so they all become a row in a four by eight tensor. So here's

20:29.840 --> 20:36.480
where I'm printing them. When I sample a batch xb and yb, the inputs, the transformer now are,

20:37.600 --> 20:46.800
the input x is the four by eight tensor, four rows of eight columns. And each one of these is a

20:46.800 --> 20:53.840
chunk of the training set. And then the targets here are in the associated array y, and they will

20:53.840 --> 21:00.400
come in to the transformer all the way at the end to create the loss function. So they will give us

21:00.400 --> 21:07.200
the correct answer for every single position inside x. And then these are the four independent rows.

21:07.200 --> 21:15.200
So spelled out as we did before, this four by eight array contains a total of 32 examples,

21:15.200 --> 21:23.200
and they're completely independent as far as the transformer is concerned. So when the input is

21:23.200 --> 21:30.560
24, the target is 43, or rather 43 here in the y array. When the input is 24, 43, the target is 58.

21:30.560 --> 21:39.040
When the input is 24, 43, 58, the target is five, etc. Or like when it is 52, 58, 1, the target is 58.

21:39.040 --> 21:44.880
Right, so you can sort of see this spelled out. These are the 32 independent examples packed in

21:44.880 --> 21:52.240
to a single batch of the input x, and then the desired targets are in y. And so now this

21:52.240 --> 21:59.600
integer tensor of x is going to feed into the transformer. And that transformer is going to

21:59.600 --> 22:05.760
simultaneously process all these examples and then look up the correct integers to predict in every

22:05.760 --> 22:11.200
one of these positions in the tensor y. Okay, so now that we have our batch of input that we'd like

22:11.200 --> 22:16.240
to feed into a transformer, let's start basically feeding this into neural networks. Now we're

22:16.240 --> 22:20.480
going to start off with the simplest possible neural network, which in this case is the

22:20.480 --> 22:24.560
simplest possible neural network, which in the case of language modeling, in my opinion, is the

22:24.560 --> 22:29.360
bi-gram language model. And we've covered the bi-gram language model in my Make More series in a lot

22:29.360 --> 22:34.560
of depth. And so here I'm going to sort of go faster and let's just implement the PyTorch module

22:34.560 --> 22:41.360
directly that implements the bi-gram language model. So I'm importing the PyTorch nn module

22:42.800 --> 22:47.680
for reproducibility. And then here I'm constructing a bi-gram language model, which is a subclass of

22:47.680 --> 22:54.480
the input and then I'm calling it and I'm passing in the inputs and the targets. And I'm just printing.

22:55.120 --> 23:00.080
Now when the inputs and targets come here, you see that I'm just taking the index, the inputs

23:00.640 --> 23:05.120
x here, which I renamed to idx. And I'm just passing them into this token embedding table.

23:06.000 --> 23:10.880
So what's going on here is that here in the constructor, we are creating a token embedding

23:10.880 --> 23:17.920
table and it is of size vocab size by vocab size. And we're using an end-end embedding,

23:17.920 --> 23:23.280
which is a very thin wrapper around basically a tensor of shape vocab size by vocab size.

23:24.000 --> 23:29.520
And what's happening here is that when we pass idx here, every single integer in our input

23:29.520 --> 23:34.800
is going to refer to this embedding table and is going to pluck out a row of that embedding table

23:34.800 --> 23:40.800
corresponding to its index. So 24 here will go to the embedding table and will pluck out the 24

23:40.800 --> 23:47.520
row. And then 43 will go here and pluck out the 43rd row, etc. And then PyTorch is going to arrange

23:47.520 --> 23:56.720
all of this into a batch by time by channel tensor. In this case, batch is 4, time is 8, and C, which is

23:56.720 --> 24:03.680
the channels is vocab size or 65. And so we're just going to pluck out all those rows, arrange them

24:03.680 --> 24:09.520
in a b by t by c. And now we're going to interpret this as the logits, which are basically the scores

24:09.520 --> 24:14.880
for the next character in a sequence. And so what's happening here is we are predicting what comes

24:14.880 --> 24:20.720
next based on just the individual identity of a single token. And you can do that because,

24:21.680 --> 24:25.520
I mean, currently the tokens are not talking to each other, and they're not seeing any context,

24:25.520 --> 24:31.760
except for they're just seeing themselves. So I'm a token number five. And then I can actually

24:31.760 --> 24:36.320
make pretty decent predictions about what comes next, just by knowing that I'm token five, because

24:36.320 --> 24:43.840
some characters follow other characters in technical scenarios. So we saw a lot of this in a lot more

24:43.840 --> 24:49.520
depth in the Make More series. And here, if I just run this, then we currently get the predictions,

24:49.520 --> 24:55.920
the scores, the logits for every one of the four by eight positions. Now that we've made predictions

24:55.920 --> 25:00.560
about what comes next, we'd like to evaluate the loss function. And so in Make More series, we saw

25:00.560 --> 25:06.560
that a good way to measure a loss or like a quality of the predictions is to use the negative log

25:06.560 --> 25:11.600
likelihood loss, which is also implemented in PyTorch under the name cross entropy. So what we'd

25:11.600 --> 25:18.480
like to do here is loss is the cross entropy on the predictions and the targets. And so this

25:18.480 --> 25:23.840
measures the quality of the logits with respect to the targets. In other words, we have the identity

25:23.840 --> 25:29.680
of the next character. So how well are we predicting the next character based on the logits? And

25:29.680 --> 25:37.920
intuitively, the correct dimension of logits, depending on whatever the target is, should have

25:37.920 --> 25:43.520
a very high number. And all the other dimensions should be very low number. Now the issue is that

25:43.520 --> 25:48.320
this won't actually, this is what we want. We want to basically output the logits and the loss.

25:50.800 --> 25:57.120
This is what we want, but unfortunately, this won't actually run. We get an error message.

25:57.120 --> 26:03.680
But intuitively, we want to measure this. Now when we go to the PyTorch cross entropy

26:04.560 --> 26:11.040
documentation here, we're trying to call the cross entropy in its functional form. So that

26:11.040 --> 26:16.000
means we don't have to create like a module for it. But here when we go to the documentation,

26:16.640 --> 26:21.120
you have to look into the details of how PyTorch expects these inputs. And basically the issue

26:21.120 --> 26:26.320
here is PyTorch expects, if you have multi dimensional input, which we do, because we have a

26:26.320 --> 26:33.600
B by T by C tensor, then it actually really wants the channels to be the second dimension here.

26:34.640 --> 26:43.920
So basically, it wants a B by C by T instead of a B by T by C. And so it's just the details of how

26:43.920 --> 26:51.120
PyTorch treats these kinds of inputs. And so we don't actually want to deal with that. So what

26:51.120 --> 26:55.280
we're going to do instead is we need to basically reshape our logits. So here's what I like to do.

26:55.280 --> 27:00.880
I like to take basically give names to the dimensions. So logits.shape is B by T by C and

27:00.880 --> 27:08.160
unpack those numbers. And then let's say that logits equals logits.view. And we want it to be a B

27:08.160 --> 27:16.240
times T by C. So just a two dimensional array. Right. So we're going to take all the, we're going

27:16.240 --> 27:22.000
to take all of these positions here, and we're going to stretch them out in a one dimensional

27:22.000 --> 27:28.240
sequence and preserve the channel dimension as the second dimension. So we're just kind of

27:28.240 --> 27:31.680
like stretching out the array. So it's two dimensional. And in that case, it's going to

27:31.680 --> 27:37.360
better conform to what PyTorch sort of expects in its dimensions. Now we have to do the same to

27:37.360 --> 27:46.160
targets, because currently targets are of shape B by T. And we want it to be just B times T.

27:46.160 --> 27:51.920
So one dimensional. Now, alternatively, you could always still just do minus one, because PyTorch

27:51.920 --> 27:55.920
will guess what this should be if you want to lay it out. But let me just be explicit and say

27:55.920 --> 28:02.720
view times T. Once we've reshaped this, it will match the cross entropy case. And then we should

28:02.720 --> 28:11.040
be able to evaluate our loss. Okay, so that right now, and we can do loss. And so currently we see

28:11.040 --> 28:18.720
that the loss is 4.87. Now, because we have 65 possible vocabulary elements, we can actually

28:18.720 --> 28:24.560
guess at what the loss should be. And in particular, we covered negative log likelihood in a lot of

28:24.560 --> 28:33.920
detail, we are expecting log or ln of one over 65 and negative of that. So we're expecting the

28:33.920 --> 28:40.480
loss to be about 4.17, but we're getting 4.87. And so that's telling us that the initial prediction

28:40.480 --> 28:45.600
is are not super diffuse, they've got a little bit of entropy. And so we're guessing wrong.

28:46.880 --> 28:54.560
So yes, but actually we are able to evaluate the loss. Okay, so now that we can evaluate the quality

28:54.560 --> 28:59.760
of the model on some data, we'd like to also be able to generate from the model. So let's do the

28:59.760 --> 29:04.240
generation. Now I'm going to go again a little bit faster here because I covered all this already

29:04.240 --> 29:14.240
in the previous videos. So here's a generate function for the model. So we take some, we take

29:14.240 --> 29:23.040
the same kind of input IDX here. And basically, this is the current context of some characters

29:23.040 --> 29:30.160
in a batch, in some batch. So it's also b by t. And the job of generate is to basically take this

29:30.160 --> 29:34.640
b by t and extend it to be b by t plus one plus two plus three. And so it's just basically it

29:34.640 --> 29:40.640
continues the generation in all the batch dimensions in the time dimension. So that's its job. And we'll

29:40.640 --> 29:45.680
do that for max new tokens. So you can see here on the bottom, there's going to be some stuff here.

29:45.680 --> 29:51.200
But on the bottom, whatever is predicted is concatenated on top of the previous IDX

29:51.920 --> 29:55.600
along the first dimension, which is the time dimension to create a b by t plus one.

29:55.600 --> 30:01.520
So that becomes a new IDX. So the job of generate is to take a b by t and make it a b by t plus

30:01.520 --> 30:07.040
one plus two plus three, as many as we want max new tokens. So this is the generation from the

30:07.040 --> 30:11.920
model. Now inside the generation, what are we doing? We're taking the current indices,

30:12.560 --> 30:19.280
we're getting the predictions. So we get those are in the logits. And then the loss here is going

30:19.280 --> 30:24.080
to be ignored because we're not we're not using that. And we have no targets that are sort of

30:24.080 --> 30:29.840
ground truth targets that we're going to be comparing with. Then once we get the logits,

30:29.840 --> 30:36.720
we are only focusing on the last step. So instead of a b by t by C, we're going to pluck out the

30:36.720 --> 30:41.040
negative one, the last element in the time dimension, because those are the predictions for

30:41.040 --> 30:46.800
what comes next. So that gives us the logits, which we then cover to probabilities via softmax.

30:47.360 --> 30:51.760
And then we use torsion multinomial to sample from those probabilities. And we ask PyTorch to give

30:51.760 --> 31:00.080
us one sample. And so IDX next will become a b by one, because each one of the batch dimensions,

31:00.080 --> 31:04.560
we're going to have a single prediction for what comes next. So this num samples equals one,

31:04.560 --> 31:10.400
will make this be a one. And then we're going to take those integers that come from the sampling

31:10.400 --> 31:15.360
process, according to the probability distribution given here. And those integers got just concatenated

31:15.360 --> 31:22.000
on top of the current sort of like running stream of integers. And this gives us a b by t plus one.

31:22.000 --> 31:28.080
And then we can return that. Now, one thing here is, you see how I'm calling self of IDX,

31:28.080 --> 31:33.200
which will end up going to the forward function. I'm not providing any targets. So currently,

31:33.200 --> 31:39.840
this would give an error because targets is sort of like not given. So targets has to be optional.

31:39.840 --> 31:47.120
So targets is none by default. And then if targets is none, then there's no loss to create. So it's

31:47.120 --> 31:53.920
just losses none. But else, all of this happens and we can create a loss. So this will make it so

31:55.520 --> 31:58.640
if we have the targets, we provide them and get a loss. If we have no targets,

31:58.640 --> 32:03.280
it will just get the logits. So this here will generate from the model.

32:03.280 --> 32:12.960
And let's take that for a ride now. So I have another code chunk here, which will generate for

32:12.960 --> 32:18.400
the model from the model. And okay, this is kind of crazy. So maybe let me let me break this down.

32:19.200 --> 32:28.720
So these are the IDX, right? I'm creating a batch will be just one time will be just one.

32:28.720 --> 32:35.520
So I'm creating a little one by one tensor, and it's holding a zero. And the d type, the data type

32:35.520 --> 32:41.920
is integer. So zero is going to be how we kick off the generation. And remember that zero is

32:42.960 --> 32:47.920
the element standing for a new line character. So it's kind of like a reasonable thing to feed in

32:47.920 --> 32:54.080
as the very first character in a sequence to be the new line. So it's going to be IDX,

32:54.080 --> 32:59.360
which we're going to feed in here. Then we're going to ask for 100 tokens. And then end that

32:59.360 --> 33:07.200
generate will continue that. Now, because generate works on the level of batches, we then have to

33:07.200 --> 33:15.280
index into the zero throw to basically unplug the single batch dimension that exists. And then

33:16.000 --> 33:21.600
that gives us a time steps, just a one dimensional array of all the indices,

33:21.600 --> 33:27.920
which we will convert to simple Python list from pytorch tensor, so that that can feed into our

33:27.920 --> 33:35.520
decode function and convert those integers into text. So let me bring this back. And we're generating

33:35.520 --> 33:42.640
100 tokens, let's run. And here's the generation that we achieved. So obviously, as garbage,

33:42.640 --> 33:46.720
and the reason it's garbage is because this is totally random model. So next up, we're going to

33:46.720 --> 33:52.240
want to train this model. Now, one more thing I wanted to point out here is this function is

33:52.240 --> 33:58.000
written to be general. But it's kind of like ridiculous right now, because we're feeding in

33:58.000 --> 34:03.360
all this, we're building out this context, and we're concatenating it all, and we're always feeding it

34:03.360 --> 34:09.120
all into the model. But that's kind of ridiculous, because this is just a simple by Graham model.

34:09.120 --> 34:14.400
So to make, for example, this prediction about K, we only needed this w. But actually, what we

34:14.400 --> 34:19.680
fed into the model is we fed the entire sequence. And then we only looked at the very last piece,

34:19.840 --> 34:25.360
and predicted K. So the only reason I'm writing it in this way is because right now this is a

34:25.360 --> 34:32.080
by Graham model, but I'd like to keep this function fixed. And I'd like it to work later when our

34:32.080 --> 34:37.920
characters actually basically look further in the history. And so right now the history is not

34:37.920 --> 34:43.760
used. So this looks silly. But eventually the history will be used. And so that's why we want to

34:43.760 --> 34:50.320
do it this way. So just a quick comment on that. So now we see that this is random. So let's train

34:50.320 --> 34:55.280
the model. So it becomes a bit less random. Okay, let's now train the model. So first, what I'm going

34:55.280 --> 35:00.960
to do is I'm going to create a pytorch optimization object. So here we are using the optimizer,

35:01.760 --> 35:07.280
Adam w. Now in a make more series, we've only ever used the casting gradient descent, the simplest

35:07.280 --> 35:12.240
possible optimizer, which you can get using the SGD instead. But I want to use Adam, which is a much

35:12.240 --> 35:18.560
more advanced and popular optimizer. And it works extremely well for typical good setting for the

35:18.560 --> 35:23.520
learning rate is roughly three negative four. But for very, very small networks, like is the case

35:23.520 --> 35:27.840
here, you can get away with much, much higher learning rates, one negative three or even higher

35:27.840 --> 35:34.320
probably. But let me create the optimizer object, which will basically take the gradients and update

35:34.320 --> 35:41.200
the parameters using the gradients. And then here, our batch size above was only four. So let me

35:41.200 --> 35:46.720
actually use something bigger, let's say 32. And then for some number of steps, we are sampling a

35:46.720 --> 35:52.400
new batch of data, we're evaluating the loss, we're zeroing out all the gradients from the previous

35:52.400 --> 35:57.520
step, getting the gradients for all the parameters, and then using those gradients to update our

35:57.520 --> 36:03.760
parameters. So typical training loop, as we saw in the make more series. So let me now run this

36:04.880 --> 36:07.920
for say 100 iterations. And let's see what kind of losses we're going to get.

36:07.920 --> 36:15.680
So we started around 4.7. And now we're going to down to like 4.6, 4.5, etc. So the optimization is

36:15.680 --> 36:22.640
definitely happening. But let's sort of try to increase number of iterations and only print at

36:22.640 --> 36:29.680
the end. Because we probably will not train for longer. Okay, so we're down to 3.6, roughly.

36:29.680 --> 36:36.560
Okay, so we're down to 3.6, roughly. Roughly down to three.

36:41.440 --> 36:43.040
This is the most janky optimization.

36:47.120 --> 36:54.080
Okay, it's working. Let's just do 10,000. And then from here, we want to copy this.

36:55.200 --> 36:58.640
And hopefully, we're going to get something reasonable. And of course, it's not going to be

36:58.640 --> 37:04.800
Shakespeare from by-gram model. But at least we see that the loss is improving. And hopefully,

37:04.800 --> 37:10.240
we're expecting something a bit more reasonable. Okay, so we're down at about 2.5 ish. Let's see

37:10.240 --> 37:17.440
what we get. Okay, dramatic improvement, certainly on what we had here. So let me just increase the

37:17.440 --> 37:23.600
number of tokens. Okay, so we see that we're starting to get something at least like reasonable ish.

37:23.600 --> 37:32.640
Certainly not Shakespeare. But the model is making progress. So that is the simplest possible model.

37:34.000 --> 37:40.240
So now what I'd like to do is, obviously, that this is a very simple model because the tokens

37:40.240 --> 37:44.960
are not talking to each other. So given the previous context of whatever was generated,

37:44.960 --> 37:48.640
we're only looking at the very last character to make the predictions about what's next.

37:48.640 --> 37:55.040
So now these tokens have to start talking to each other and figuring out what is in the context

37:55.040 --> 37:58.400
so that they can make better predictions for what comes next. And this is how we're going to kick

37:58.400 --> 38:03.360
off the transformer. Okay, so next, I took the code that we developed in this Jupyter Notebook

38:03.360 --> 38:09.280
and I converted it to be a script. And I'm doing this because I just want to simplify our intermediate

38:09.280 --> 38:15.040
work, which is just the final product that we have at this point. So in the top here, I put all

38:15.040 --> 38:19.360
the hyperparameters that we find. I introduced a few and I'm going to speak to that in a little bit.

38:20.000 --> 38:25.920
Otherwise, a lot of this should be recognizable. Reproducibility, read data, get the encoder and

38:25.920 --> 38:33.520
the decoder, create the train into splits. I use the kind of like data loader that gets a batch

38:33.520 --> 38:39.360
of the inputs and targets. This is new and I'll talk about it in a second. Now this is the

38:39.360 --> 38:44.240
biogram language model that we developed. And it can forward and give us a logits and loss and it

38:44.240 --> 38:49.840
can generate. And then here we are creating the optimizer and this is the training loop.

38:51.840 --> 38:56.080
So everything here should look pretty familiar. Now some of the small things that I added,

38:56.080 --> 39:02.400
number one, I added the ability to run on a GPU if you have it. So if you have a GPU, then you can

39:02.400 --> 39:07.680
this will use CUDA instead of just CPU and everything will be a lot more faster. Now when

39:07.680 --> 39:12.720
device becomes CUDA, then we need to make sure that when we load the data, we move it to device.

39:12.720 --> 39:20.080
When we create the model, we want to move the model parameters to device. So as an example,

39:20.720 --> 39:25.280
here we have the in an embedding table and it's got a dot weight inside it, which stores the

39:25.920 --> 39:30.800
sort of lookup table. So that would be moved to the GPU so that all the calculations here

39:30.800 --> 39:36.320
happen on the GPU and they can be a lot faster. And then finally here when I'm creating the context

39:36.320 --> 39:41.120
that feeds into generate, I have to make sure that I create on the device. Number two, what I

39:41.120 --> 39:50.800
introduced is the fact that here in the training loop, here I was just printing the loss dot item

39:51.760 --> 39:55.680
inside the training loop. But this is a very noisy measurement of the current loss because every

39:55.680 --> 40:03.120
batch will be more or less lucky. And so what I want to do usually is I have an estimate loss

40:03.120 --> 40:11.840
function and the estimate loss basically then goes up here and it averages up the loss over

40:11.840 --> 40:17.840
multiple batches. So in particular, we're going to iterate eval-idder times and we're going to

40:17.840 --> 40:22.800
basically get our loss and then we're going to get the average loss for both splits. And so this

40:22.800 --> 40:28.400
will be a lot less noisy. So here when we call the estimate loss, we're going to report the

40:28.400 --> 40:34.160
pretty accurate train and validation loss. Now when we come back up, you'll notice a few things

40:34.160 --> 40:39.680
here. I'm setting the model to evaluation phase and down here I'm resetting it back to training

40:39.680 --> 40:45.200
phase. Now right now for our model as is, this doesn't actually do anything because the only thing

40:45.200 --> 40:54.000
inside this model is this nm.embedding and this network would behave both, would behave the same

40:54.000 --> 40:58.640
in both evaluation mode and training mode. We have no dropout layers, we have no bathroom layers,

40:58.640 --> 41:04.560
etc. But it is a good practice to think through what mode your neural network is in because some

41:04.560 --> 41:11.760
layers will have different behavior at inference time or training time. And there's also this

41:11.760 --> 41:16.000
context manager torched up no grad and this is just telling PyTorch that everything that happens

41:16.000 --> 41:21.760
inside this function, we will not call dot backward all. And so PyTorch can be a lot more

41:21.760 --> 41:26.720
efficient with its memory use because it doesn't have to store all the intermediate variables

41:26.720 --> 41:31.200
because we're never going to call backward. And so it can be a lot more memory efficient in that

41:31.200 --> 41:36.400
way. So also a good practice to tell PyTorch when we don't intend to do back propagation.

41:37.600 --> 41:44.320
So right now this script is about 120 lines of code off and that's kind of our starter code.

41:45.200 --> 41:49.840
I'm calling it by gram dot pi and I'm going to release it later. Now running this script

41:49.840 --> 41:56.480
gives us output in the terminal and it looks something like this. It basically, as I ran this

41:56.480 --> 42:02.080
code, it was giving me the train loss and val loss and we see that we convert to somewhere around 2.5

42:02.720 --> 42:06.800
with the PyGram model. And then here's the sample that we produced at the end.

42:08.320 --> 42:12.560
And so we have everything packaged up in the script and we're in a good position now to iterate

42:12.560 --> 42:18.240
on this. Okay, so we are almost ready to start writing our very first self attention block for

42:18.240 --> 42:25.520
processing these tokens. Now, before we actually get there, I want to get you used to a mathematical

42:25.520 --> 42:30.240
trick that is used in the self attention inside a transformer and it's really just like at the

42:30.240 --> 42:35.840
heart of an efficient implementation of self attention. And so I want to work with this toy

42:35.840 --> 42:40.000
example to just get you used to this operation and then it's going to make it much more clear

42:40.000 --> 42:48.400
once we actually get to it in the script again. So let's create a b by t by c where b, t and c are

42:48.400 --> 42:54.880
just four, eight and two in this toy example. And these are basically channels and we have batches

42:54.880 --> 43:00.320
and we have the time component and we have some information at each point in the sequence. So c.

43:01.920 --> 43:07.360
Now what we would like to do is we would like these tokens. So we have up to eight tokens here

43:07.360 --> 43:11.760
in a batch and these eight tokens are currently not talking to each other and we would like them to

43:11.760 --> 43:18.160
talk to each other. We'd like to couple them. And in particular, we don't we we want to couple them

43:18.160 --> 43:24.160
in a very specific way. So the token for example at the fifth location, it should not communicate

43:24.160 --> 43:29.760
with tokens in the sixth, seventh and eighth location, because those are future tokens in the

43:29.760 --> 43:34.800
sequence. The token on the fifth location should only talk to the one in the fourth, third, second

43:34.800 --> 43:40.640
and first. So it's only so information only flows from previous context to the current time step.

43:41.200 --> 43:45.120
And we cannot get any information from the future because we are about to try to predict the future.

43:46.400 --> 43:53.200
So what is the easiest way for tokens to communicate? Okay, the easiest way I would say

43:53.200 --> 43:57.760
is okay, if we are up to if we're a fifth token, and I'd like to communicate with my past,

43:57.760 --> 44:03.600
the simplest way we can do that is to just do a weight is to just do an average of all the

44:03.600 --> 44:08.320
of all the preceding elements. So for example, if I'm the fifth token, I would like to take the

44:08.320 --> 44:14.640
channels that make up that that are information at my step. But then also the channels from the

44:14.640 --> 44:19.520
fourth step, third step, second step in the first step, I'd like to average those up. And then that

44:19.520 --> 44:24.320
would become sort of like a feature vector that summarizes me in the context of my history.

44:25.120 --> 44:29.680
Now, of course, just doing a sum or like an average is an extremely weak form of interaction,

44:29.680 --> 44:35.040
like this communication is extremely lossy. We've lost a ton of information about spatial

44:35.040 --> 44:39.200
arrangements of all those tokens. But that's okay for now, we'll see how we can bring that

44:39.200 --> 44:44.400
information back later. For now, what we would like to do is for every single batch element

44:44.400 --> 44:51.840
independently, for every teeth token in that sequence, we'd like to now calculate the average

44:51.840 --> 45:00.000
of all the vectors in all the previous tokens and also at this token. So let's write that out. I have

45:00.000 --> 45:04.960
a small snippet here. And instead of just fumbling around, let me just copy paste it and talk to it.

45:06.400 --> 45:13.120
So in other words, we're going to create X and BOW is short for bag of words, because bag of words

45:13.120 --> 45:19.440
is is kind of like a term that people use when you are just averaging up things. So this is just a

45:19.440 --> 45:24.400
bag of words. Basically, there's a word stored on every one of these eight locations, and we're

45:24.400 --> 45:29.200
doing a bag of words, just averaging. So in the beginning, we're going to say that it's just

45:29.200 --> 45:33.120
initialized at zero. And then I'm doing a for loop here. So we're not being efficient yet,

45:33.120 --> 45:37.840
that's coming. But for now, we're just iterating over all the batch dimensions independently,

45:37.840 --> 45:46.560
iterating over time. And then the previous tokens are at this batch dimension, and then

45:46.560 --> 45:54.160
everything up to and including the teeth token. Okay. So when we slice out X in this way, X-prev

45:54.160 --> 46:01.840
becomes of shape. How many T elements that were in the past. And then of course, C. So all the

46:01.840 --> 46:07.600
two dimensional information from these little tokens. So that's the previous sort of chunk of

46:08.640 --> 46:14.720
tokens from my current sequence. And then I'm just doing the average or the mean over the

46:14.720 --> 46:20.800
zero dimension. So I'm averaging out the time here. And I'm just going to get a little C one

46:20.800 --> 46:27.760
dimensional vector, which I'm going to store in X back of words. So I can run this. And this is not

46:27.760 --> 46:34.320
going to be very informative because let's see. So this is X of zero. So this is the zero batch

46:34.320 --> 46:42.320
element. And then X bow at zero. Now, you see how the at the first location here, you see that the

46:42.320 --> 46:48.320
two are equal. And that's because it's, we're just doing an average of this one token. But here,

46:48.320 --> 46:56.400
this one is now an average of these two. And now this one is an average of these three. And so on.

46:57.680 --> 47:03.920
So, and this last one is the average of all of these elements. So vertical average, just averaging

47:03.920 --> 47:10.800
up all the tokens now gives this outcome here. So this is all well and good. But this is very

47:10.800 --> 47:15.760
inefficient. Now the trick is that we can be very, very efficient about doing this using matrix

47:15.760 --> 47:20.960
multiplication. So that's the mathematical trick. And let me show you what I mean. Let's work with

47:20.960 --> 47:28.080
the toy example here. Let me run it and I'll explain. I have a simple matrix here that is a three by

47:28.080 --> 47:34.000
three of all ones, a matrix B of just random numbers, and it's a three by two, and a matrix C,

47:34.000 --> 47:39.520
which will be three by three, multiply three by two, which will give out a three by two. So here,

47:39.520 --> 47:49.680
we're just using matrix multiplication. So a multiply B gives us C. Okay, so how are these numbers

47:49.680 --> 47:57.600
in C achieved, right? So this number in the top left is the first row of A, dot product,

47:57.600 --> 48:02.880
with the first column of B. And since all the row of A right now is all just once,

48:03.680 --> 48:09.040
then the dot product here with with this column of B is just going to do a sum of these,

48:09.040 --> 48:16.000
of this column. So two plus six plus six is 14. The element here in the output of C is also the

48:16.000 --> 48:22.240
first column here, the first row of A multiplied now with the second column of B. So seven plus four

48:22.240 --> 48:28.240
plus five is 16. Now you see that there's repeating elements here. So this 14 again is because this

48:28.240 --> 48:33.680
row is again all once and it's multiplying the first column of B. So we get 14. And this one is

48:33.680 --> 48:40.880
this and so on. So this last number here is the last row dot product last column. Now the trick

48:40.880 --> 48:48.400
here is the following. This is just a boring number of, it's just a boring array of all once,

48:48.960 --> 48:55.920
but torches this function called trill, which is short for a triangular, something like that.

48:56.480 --> 49:01.040
And you can wrap it in torche dot once and we'll just return the lower triangular portion

49:01.040 --> 49:08.320
of this. Okay. So now it will basically zero out these guys here. So we just get the lower

49:08.320 --> 49:11.680
triangular part. Well, what happens if we do that?

49:14.960 --> 49:19.360
So now we'll have A like this and B like this. And now what are we getting here and C?

49:20.240 --> 49:26.080
Well, what is this number? Well, this is the first row times the first column. And because this is

49:26.080 --> 49:33.840
zeros, these elements here are now ignored. So we just get a two. And then this number here is the

49:33.840 --> 49:39.200
first row times the second column. And because these are zeros, they get ignored. And it's just

49:39.200 --> 49:44.960
seven. The seven multiplies this one. But look what happened here because this is one and then zeros,

49:45.520 --> 49:50.480
we, what ended up happening is we're just plucking out the row of this row of B. And that's what we

49:50.480 --> 49:58.720
got. Now here, we have one, one, zero. So here, one, one, zero dot product with these two columns

49:58.720 --> 50:03.920
will now give us two plus six, which is eight and seven plus four, which is 11. And because this is

50:03.920 --> 50:10.080
one, one, one, we ended up with the addition of all of them. And so basically, depending on how

50:10.080 --> 50:17.600
many ones and zeros we have here, we are basically doing a sum currently of the variable number of

50:17.600 --> 50:23.440
these rows. And that gets deposited into C. So currently, we're doing sums because these are

50:23.440 --> 50:29.920
ones, but we can also do average, right? And you can start to see how we could do average of the rows

50:29.920 --> 50:36.560
of B, sort of in incremental fashion, because we don't have to, we can basically normalize these

50:36.560 --> 50:42.000
rows so that they sum to one, and then we're going to get an average. So if we took a, and then we

50:42.000 --> 50:55.200
did a equals a divide a torch dot sum in the of a in the one dimension. And then let's keep them

50:55.200 --> 51:01.360
is true. So therefore, the broadcasting will work out. So if I rerun this, you see now that these

51:01.360 --> 51:07.520
rows now sum to one. So this row is one, this was point point five point five zero. And here we get

51:07.520 --> 51:13.600
one thirds. And now when we do a multiply B, what are we getting? Here we are just getting the first

51:13.600 --> 51:22.240
row, first row. Here now we are getting the average of the first two rows. Okay, so two and six

51:22.240 --> 51:28.400
average is four, and four and seven averages five point five. And on the bottom here, we're now getting

51:28.400 --> 51:35.360
the average of these three rows. So the average of all of elements of B are now deposited here.

51:35.360 --> 51:41.280
And so you can see that by manipulating these elements of this multiplying matrix, and then

51:41.280 --> 51:47.840
multiplying it with any given matrix, we can do these averages in this incremental fashion, because

51:47.840 --> 51:54.560
we just get and we can manipulate that based on the elements of a. Okay, so that's very convenient.

51:54.560 --> 51:59.200
So let's swing back up here and see how we can vectorize this and make it much more efficient

51:59.200 --> 52:06.880
using what we've learned. So in particular, we are going to produce an array a, but here I'm going

52:06.880 --> 52:14.560
to call it way short for weights. But this is our a. And this is how much of every row we want to

52:14.560 --> 52:18.800
average up. And it's going to be an average because you can see that these rows sum to one.

52:20.000 --> 52:28.320
So this is our a, and then our B in this example, of course, is x. So it's going to happen here now

52:28.320 --> 52:36.000
is that we are going to have an expo two. And this expo two is going to be way multiplying

52:36.720 --> 52:44.320
rx. So let's think this through way is t by t. And this is matrix multiplying in pytorch,

52:44.320 --> 52:52.640
a B by t by C. And it's giving us what shape. So pytorch will come here and it will see that

52:52.640 --> 52:58.640
these shapes are not the same. So it will create a batch dimension here. And this is a batch matrix

52:58.640 --> 53:04.320
multiply. And so it will apply this matrix multiplication in all the batch elements in

53:04.320 --> 53:10.960
parallel and individually. And then for each batch element, there will be a t by t multiplying t by

53:10.960 --> 53:23.840
C, exactly as we had below. So this will now create B by t by C. And expo two will now become

53:23.840 --> 53:34.400
identical to expo. So we can see that torch dot all close of expo and expo two should be true.

53:34.400 --> 53:45.200
Now, so this kind of like convinces us that these are in fact the same. So expo and expo two, if I

53:45.200 --> 53:52.240
just print them. Okay, we're not going to be able to, okay, we're not going to be able to just stare

53:52.240 --> 53:58.880
it down. But let me try expo basically just at the zeroth element and expo two at the zeroth element.

53:58.880 --> 54:04.000
So just the first batch. And we should see that this and that should be identical, which they are.

54:05.360 --> 54:10.320
Right. So what happened here, the trick is, we were able to use batch matrix multiply

54:10.960 --> 54:18.720
to do this aggregation, really. And it's a weighted aggregation. And the weights are specified in this

54:19.440 --> 54:27.440
t by t array. And we're basically doing weighted sums. And these weighted sums are according to

54:27.440 --> 54:33.680
the weights inside here that take on sort of this triangular form. And so that means that a token at

54:33.680 --> 54:41.120
the t dimension will only get sort of information from the tokens preceding it. So that's exactly

54:41.120 --> 54:46.400
what we want. And finally, I would like to rewrite it in one more way. And we're going to see why

54:46.400 --> 54:52.240
that's useful. So this is the third version. And it's also identical to the first and second.

54:52.240 --> 55:01.440
But let me talk through it. It uses softmax. So trill here is this matrix, lower triangular ones.

55:02.640 --> 55:09.440
Way begins as all zero. Okay, so if I just print way in the beginning, it's all zero.

55:10.480 --> 55:18.480
Then I use masked fill. So what this is doing is way that masked fill, it's all zeros. And I'm

55:18.480 --> 55:24.640
saying, for all the elements where trill is equals equals zero, make them be negative infinity.

55:25.360 --> 55:30.960
So all the elements where trill is zero will become negative infinity now. So this is what we get.

55:32.160 --> 55:40.080
And then the final one here is softmax. So if I take a softmax along every single, so dim is

55:40.080 --> 55:45.200
negative one, so along every single row, if I do a softmax, what is that going to do?

55:45.200 --> 55:55.920
Well, softmax is also like a normalization operation, right? And so spoiler alert,

55:55.920 --> 56:02.560
you get the exact same matrix. Let me bring back the softmax. And recall that in softmax,

56:02.560 --> 56:07.280
we're going to exponentiate every single one of these. And then we're going to divide by the sum.

56:08.160 --> 56:12.880
And so if we exponentiate every single element here, we're going to get a one. And here we're

56:12.880 --> 56:18.640
going to get basically zero, zero, zero, zero, everywhere else. And then when we normalize,

56:18.640 --> 56:24.160
we just get one. Here we're going to get one, one, and then zeros. And then softmax will,

56:24.160 --> 56:30.400
again, divide, and this will give us point five point five and so on. And so this is also the

56:30.400 --> 56:35.840
same way to produce this mask. Now the reason that this is a bit more interesting, and the reason

56:35.840 --> 56:44.080
we're going to end up using it in self attention is that these weights here begin with zero. And

56:44.080 --> 56:48.960
you can think of this as like an interaction strength or like an affinity. So basically,

56:48.960 --> 56:55.680
it's telling us how much of each token from the past do we want to aggregate an average up.

56:57.040 --> 57:02.720
And then this line is saying tokens from the past cannot communicate by setting them to

57:02.720 --> 57:06.800
negative infinity. We're saying that we will not aggregate anything from those tokens.

57:08.240 --> 57:12.400
And so basically, this then goes through softmax and through the weighted, and this is the aggregation

57:12.400 --> 57:20.000
through matrix multiplication. And so what this is now is you can think of these as these zeros

57:20.000 --> 57:26.240
are currently just set by us to be zero. But a quick preview is that these affinities between

57:26.240 --> 57:31.120
the tokens are not going to be just constant at zero, they're going to be data dependent.

57:31.120 --> 57:35.600
These tokens are going to start looking at each other. And some tokens will find other tokens

57:35.600 --> 57:40.960
more or less interesting. And depending on what their values are, they're going to find each other

57:40.960 --> 57:45.840
interesting to different amounts. And I'm going to call those affinities, I think. And then here we

57:45.840 --> 57:51.680
are saying the future cannot communicate with the past, we're going to clamp them. And then when we

57:51.680 --> 57:56.960
normalize and some we're going to aggregate sort of their values, depending on how interesting

57:56.960 --> 58:03.280
they find each other. And so that's the preview for self attention. And basically, long story short

58:03.280 --> 58:08.400
from this entire section is that you can do weighted aggregations of your past elements

58:09.440 --> 58:16.640
by having by using matrix multiplication of a lower triangular fashion. And then the elements

58:16.640 --> 58:22.480
here in the lower triangular part are telling you how much of each element fuses into this position.

58:22.480 --> 58:26.640
So we're going to use this trick now to develop the self attention block. So first,

58:26.640 --> 58:31.520
let's get some quick preliminaries out of the way. First, the thing I'm kind of bothered by is that

58:31.520 --> 58:35.280
you see how we're passing in vocab size into the constructor. There's no need to do that because

58:35.280 --> 58:40.000
vocab size is already defined up top as a global variable. So there's no need to pass this stuff

58:40.000 --> 58:46.160
around. Next, what I want to do is I don't want to actually create, I want to create like a level

58:46.160 --> 58:52.000
of indirection here, where we don't directly go to the embedding for the logits. But instead,

58:52.000 --> 58:56.480
but instead we go through this intermediate phase because we're going to start making that bigger.

58:57.040 --> 59:02.800
So let me introduce a new variable n embed. It's short for number of embedding dimensions.

59:03.520 --> 59:11.120
So n embed here will be say 32. That was the suggestion from GitHub co-piled, by the way.

59:11.760 --> 59:18.160
It also suggested 32, which is a good number. So this is an embedding table and only 32 dimensional

59:18.160 --> 59:23.920
embeddings. So then here, this is not going to give us logits directly. Instead, this is going to

59:23.920 --> 59:28.720
give us token embeddings. That's what I'm going to call it. And then to go from the token embeddings

59:28.720 --> 59:34.320
to the logits, we're going to need a linear layer. So self.lmhead, let's call it short for

59:34.320 --> 59:41.040
language modeling head, is nn linear from n embed up to vocab size. And then when we swing over here,

59:41.040 --> 59:46.720
we're actually going to get the logits by exactly what the co-pilot says. Now we have to be careful

59:46.720 --> 59:53.840
here because this C and this C are not equal. This is n embed C and this is vocab size.

59:54.880 --> 01:00:01.440
So let's just say that n embed is equal to C. And then this just creates one spurious layer of

01:00:01.440 --> 01:00:13.600
interaction through a linear layer. But this should basically run. So we see that this runs

01:00:13.600 --> 01:00:18.000
and this currently looks kind of spurious, but we're going to build on top of this.

01:00:18.640 --> 01:00:24.400
Now next up, so far we've taken these indices and we've encoded them based on the identity of the

01:00:25.040 --> 01:00:31.520
tokens inside IDX. The next thing that people very often do is that we're not just encoding the

01:00:31.520 --> 01:00:36.240
identity of these tokens, but also their position. So we're going to have a second position

01:00:36.240 --> 01:00:43.360
embedding table here. So salt that position embedding table is an embedding of block size by n embed.

01:00:43.920 --> 01:00:48.320
And so each position from zero to block size minus one will also get its own embedding vector.

01:00:49.360 --> 01:00:56.560
And then here, first let me decode b by t from IDX.shape. And then here we're also going to have a

01:00:56.560 --> 01:01:02.000
pause embedding, which is the positional embedding. And these are this is tordash arrange. So this will

01:01:02.000 --> 01:01:07.680
be basically just integers from zero to t minus one. And all of those integers from zero to t

01:01:07.680 --> 01:01:14.880
minus one get embedded through the table to create a t by c. And then here this gets renamed to just

01:01:14.880 --> 01:01:20.560
say x and x will be the addition of the token embeddings with the positional embeddings.

01:01:21.760 --> 01:01:27.760
And here the broadcasting note will work out. So b by t by c plus t by c, this gets right aligned

01:01:27.760 --> 01:01:34.400
in new dimension of one gets added and it gets broadcasted across batch. So at this point x holds

01:01:34.400 --> 01:01:40.240
not just the token identities, but the positions at which these tokens occur. And this is currently

01:01:40.240 --> 01:01:44.000
not that useful because of course we just have a simple by-grain model. So it doesn't matter if

01:01:44.000 --> 01:01:48.560
you're in the fifth position, the second position or wherever, it's all translation invariant at this

01:01:48.560 --> 01:01:54.080
stage. So this information currently wouldn't help. But as we work on the self-attention block,

01:01:54.080 --> 01:02:02.560
we'll see that this starts to matter. Okay, so now we get the crux of self-attention. So this is

01:02:02.560 --> 01:02:07.440
probably the most important part of this video to understand. We're going to implement a small

01:02:07.440 --> 01:02:13.040
self-attention for a single individual head as they're called. So we start off with where we were.

01:02:13.040 --> 01:02:18.160
So all of this code is familiar. So right now I'm working with an example where I change the number

01:02:18.160 --> 01:02:25.600
of channels from 2 to 32. So we have a 4 by 8 arrangement of tokens and the information

01:02:25.600 --> 01:02:29.920
of each token is currently 32 dimensional, but we just are working with random numbers.

01:02:31.200 --> 01:02:38.880
Now we saw here that the code as we had it before does a simple weight, simple average

01:02:38.880 --> 01:02:44.080
of all the past tokens and the current token. So it's just the previous information and the

01:02:44.080 --> 01:02:48.560
current information is just being mixed together in an average. And that's what this code currently

01:02:48.560 --> 01:02:54.320
achieves. And it does so by creating this lower triangular structure, which allows us to mask out

01:02:54.320 --> 01:03:01.280
this weight matrix that we create. So we mask it out and then we normalize it. And currently,

01:03:02.000 --> 01:03:08.240
when we initialize the affinities between all the different sort of tokens or nodes, I'm going to

01:03:08.240 --> 01:03:12.960
use those terms interchangeably. So when we initialize the affinities between all the different

01:03:12.960 --> 01:03:19.440
tokens to be zero, then we see that way gives us this structure where every single row has these

01:03:21.040 --> 01:03:26.960
uniform numbers. And so that's what that's what then in this matrix multiply makes it so that

01:03:26.960 --> 01:03:35.200
we're doing a simple average. Now, we don't actually want this to be all uniform, because

01:03:35.200 --> 01:03:40.560
different tokens will find different other tokens more or less interesting, and we want that to be

01:03:40.560 --> 01:03:46.240
data dependent. So for example, if I'm a vowel, then maybe I'm looking for consonants in my past,

01:03:46.240 --> 01:03:50.000
and maybe I want to know what those consonants are, and I want that information to flow to me.

01:03:51.120 --> 01:03:56.240
And so I want to now gather information from the past, but I want to do it in a data dependent way.

01:03:56.240 --> 01:04:01.120
And this is the problem that self attention solves. Now the way self attention solves this

01:04:01.120 --> 01:04:07.840
is the following. Every single node or every single token at each position will emit two vectors.

01:04:07.840 --> 01:04:16.240
It will emit a query, and it will emit a key. Now the query vector roughly speaking is what am I

01:04:16.240 --> 01:04:23.200
looking for? And the key vector roughly speaking is what do I contain? And then the way we get

01:04:23.200 --> 01:04:29.840
affinities between these tokens now in a sequence is we basically just do a dot product between the

01:04:29.840 --> 01:04:36.960
keys and the queries. So my query dot products with all the keys of all the other tokens, and

01:04:36.960 --> 01:04:46.640
that dot product now becomes way. And so if the key and the query are sort of aligned, they will

01:04:46.640 --> 01:04:52.960
interact to a very high amount, and then I will get to learn more about that specific token, as

01:04:52.960 --> 01:05:01.680
opposed to any other token in the sequence. So let's implement this now. We're going to implement

01:05:01.680 --> 01:05:10.400
a single what's called head of self attention. So this is just one head. There's a hyper parameter

01:05:10.400 --> 01:05:16.560
involved with these heads, which is the head size. And then here I'm initializing linear modules,

01:05:16.560 --> 01:05:21.040
and I'm using biosecuals false. So these are just going to apply a matrix multiply with some fixed

01:05:21.040 --> 01:05:31.120
weights. And now let me produce a key and q, k and q by forwarding these modules on x. So the

01:05:31.120 --> 01:05:39.920
size of this will now become b by t by 16, because that is the head size. And the same here b by t by 16.

01:05:45.600 --> 01:05:50.720
So this being that size. So you see here that when I forward this linear on top of my x,

01:05:51.440 --> 01:05:56.880
all the tokens in all the positions in the b by t arrangement, all of them in parallel and

01:05:56.880 --> 01:06:03.280
independently produce a key and a query. So no communication has happened yet. But the communication

01:06:03.280 --> 01:06:10.080
comes now, all the queries will dot product with all the keys. So basically what we want is we want

01:06:10.080 --> 01:06:17.200
way now, or the affinities between these to be query multiplying key. But we have to be careful

01:06:17.200 --> 01:06:23.280
with we can't make this multiply this we actually need to transpose k. But we have to be also careful

01:06:23.280 --> 01:06:30.080
because these are when you have the batch dimension. So in particular, we want to transpose the last

01:06:30.080 --> 01:06:36.080
two dimensions, dimension negative one and dimension negative two. So negative two, negative one.

01:06:37.360 --> 01:06:42.800
And so this matrix multiply now will basically do the following b by t by 16.

01:06:42.800 --> 01:06:52.000
Matrix multiplies b by 16 by t to give us b by t by t.

01:06:54.320 --> 01:07:01.840
Right. So for every row of B, we're now going to have a t square matrix giving us the affinities.

01:07:01.840 --> 01:07:07.760
And these are now the way. So they're not zeros. They are now coming from this dot product between

01:07:07.760 --> 01:07:14.720
the keys and the queries. So this can now run, I can run this. And the weighted aggregation now

01:07:14.720 --> 01:07:19.280
is a function in a data band and manner between the keys and queries of these nodes.

01:07:20.160 --> 01:07:28.720
So just inspecting what happened here. The way takes on this form. And you see that before way was

01:07:28.720 --> 01:07:33.440
just a constant. So it was applied in the same way to all the batch elements. But now every

01:07:33.440 --> 01:07:38.800
single batch elements will have different sort of way because every single batch element contains

01:07:38.800 --> 01:07:44.720
different tokens at different positions. And so this is not data dependent. So when we look at

01:07:44.720 --> 01:07:51.280
just the zero row, for example, in the input, these are the weights that came out. And so you can

01:07:51.280 --> 01:07:57.600
see now that they're not just exactly uniform. And in particular, as an example here for the last row,

01:07:57.600 --> 01:08:02.480
this was the eighth token. And the eighth token knows what content it has. And it knows at what

01:08:02.480 --> 01:08:09.280
position it's in. And now the eighth token based on that creates a query. Hey, I'm looking for this

01:08:09.280 --> 01:08:13.920
kind of stuff. I'm a vowel, I'm on the eighth position, I'm looking for any consonants at

01:08:13.920 --> 01:08:21.120
positions up to four. And then all the nodes get to emit keys. And maybe one of the channels could be

01:08:21.120 --> 01:08:27.680
I am a consonant and I am in a position up to four. And that key would have a high number in

01:08:27.680 --> 01:08:32.080
that specific channel. And that's how the query and the key when they dot product, they can find

01:08:32.080 --> 01:08:37.840
each other and create a high affinity. And when they have a high affinity, like say, this token

01:08:37.840 --> 01:08:44.160
was pretty interesting to to this eighth token. When they have a high affinity, then through the

01:08:44.160 --> 01:08:49.760
softmax, I will end up aggregating a lot of its information into my position. And so I'll get to

01:08:49.760 --> 01:08:57.200
learn a lot about it. Now, just this we're looking at way after this has already happened.

01:08:57.200 --> 01:09:02.480
Let me erase this operation as well. So let me erase the masking and the softmax, just to show you

01:09:02.480 --> 01:09:08.160
the under the hood internals and how that works. So without the masking and the softmax way comes

01:09:08.160 --> 01:09:13.600
out like this, right? This is the outputs of the top products. And these are the raw outputs, and

01:09:13.600 --> 01:09:19.600
they take on values from negative, you know, two to positive two, etc. So that's the raw

01:09:19.600 --> 01:09:26.000
interactions and raw affinities between all the nodes. But now, if I'm a fifth node, I will not

01:09:26.000 --> 01:09:30.640
want to aggregate anything from the sixth node, seventh node, and the eighth node. So actually,

01:09:30.640 --> 01:09:35.680
we use the upper triangular masking. So those are not allowed to communicate.

01:09:37.520 --> 01:09:43.440
And now, we actually want to have a nice distribution. So we don't want to aggregate negative

01:09:43.440 --> 01:09:48.560
point one one of this node. That's crazy. So instead, we exponentiate and normalize. And now

01:09:48.560 --> 01:09:52.560
we get a nice distribution that sums to one. And this is telling us now in the data dependent

01:09:52.560 --> 01:09:57.200
manner, how much of information to aggregate from any of these tokens in the past.

01:09:59.120 --> 01:10:05.520
So that's way. And it's not zeros anymore. But it's calculated in this way. Now, there's one more

01:10:06.160 --> 01:10:12.000
part to a single self attention head. And that is that when we do the aggregation, we don't

01:10:12.000 --> 01:10:17.760
actually aggregate the tokens exactly. We aggregate, we produce one more value here. And we call that

01:10:17.760 --> 01:10:24.000
the value. So in the same way that we produced key inquiry, we're also going to create a value.

01:10:25.120 --> 01:10:35.760
And then here, we don't aggregate x, we calculate a V, which is just achieved by propagating this

01:10:36.400 --> 01:10:44.240
linear on top of x again. And then we output way multiplied by V. So V is the elements that we

01:10:44.240 --> 01:10:50.880
aggregate or the vectors that we aggregate instead of the raw x. And now, of course, this will make

01:10:50.880 --> 01:10:55.920
it so that the output here of the single head will be 16 dimensional, because that is the head size.

01:10:57.600 --> 01:11:01.920
So you can think of x as kind of like private information to this token, if you if you think

01:11:01.920 --> 01:11:07.200
about it that way. So x is kind of private to this token. So I'm a fifth token at some and I

01:11:07.200 --> 01:11:14.880
have some identity. And my information is kept in vector x. And now, for the purposes of the single

01:11:14.880 --> 01:11:21.680
head, here's what I'm interested in. Here's what I have. And if you find me interesting, here's what

01:11:21.680 --> 01:11:27.120
I will communicate to you. And that's stored in V. And so V is the thing that gets aggregated for

01:11:27.120 --> 01:11:34.240
the purposes of this single head between the different notes. And that's basically the self

01:11:34.240 --> 01:11:39.920
attention mechanism. This is this is what it does. There are a few notes that I would make like to

01:11:39.920 --> 01:11:45.680
make about attention. Number one, attention is a communication mechanism. You can really think

01:11:45.680 --> 01:11:50.240
about it as a communication mechanism, where you have a number of nodes in a directed graph,

01:11:50.800 --> 01:11:56.480
where basically you have edges pointed between those like this. And what happens is every node

01:11:56.480 --> 01:12:02.000
has some vector of information. And it gets to aggregate information via a weighted sum from

01:12:02.000 --> 01:12:07.840
all the nodes that point to it. And this is done in a data dependent manner. So depending on whatever

01:12:07.840 --> 01:12:13.840
data is actually stored at each node at any point in time. Now, our graph doesn't look like this.

01:12:13.840 --> 01:12:18.640
Our graph has a different structure. We have eight nodes, because the block size is eight, and there's

01:12:18.640 --> 01:12:25.680
always eight tokens. And the first node is only pointed to by itself. The second node is pointed

01:12:25.680 --> 01:12:30.960
to by the first node and itself, all the way up to the eighth node, which is pointed to by all the

01:12:30.960 --> 01:12:36.960
previous nodes and itself. And so that's the structure that our directed graph has or happens

01:12:36.960 --> 01:12:41.840
happens to have an autoregressive sort of scenario like language modeling. But in principle,

01:12:41.840 --> 01:12:45.680
attention can be applied to any arbitrary directed graph. And it's just a communication mechanism

01:12:45.680 --> 01:12:51.600
between the nodes. The second note is that note is that there's no notion of space. So attention

01:12:51.600 --> 01:12:57.520
simply acts over like a set of vectors in this graph. And so by default, these nodes have no idea

01:12:57.520 --> 01:13:02.320
where they are positioned in the space. And that's why we need to encode them positionally and sort

01:13:02.320 --> 01:13:07.360
of give them some information that is anchors to a specific position so that they sort of know

01:13:07.360 --> 01:13:11.600
where they are. And this is different than, for example, from convolution, because if you run,

01:13:11.600 --> 01:13:16.560
for example, a convolution operation over some input, there is a very specific sort of layout

01:13:16.560 --> 01:13:23.280
of the information in space and the convolutional filters sort of act in space. And so it's not

01:13:23.280 --> 01:13:28.720
like an attention. In attention is just a set of vectors out there in space, they communicate.

01:13:28.720 --> 01:13:32.560
And if you want them to have a notion of space, you need to specifically add it,

01:13:32.560 --> 01:13:38.320
which is what we've done when we calculated the relative, the positional encode encodings and

01:13:38.320 --> 01:13:42.960
added that information to the vectors. The next thing that I hope is very clear is that the elements

01:13:42.960 --> 01:13:47.200
across the batch dimension, which are independent examples, never talk to each other, they're always

01:13:47.200 --> 01:13:51.680
processed independently. And this is a batched matrix multiply that applies basically a matrix

01:13:51.680 --> 01:13:56.400
multiplication kind of in parallel across the batch dimension. So maybe it would be more accurate

01:13:56.400 --> 01:14:01.920
to say that in this analogy of a directed graph, we really have, because the batch size is four,

01:14:01.920 --> 01:14:07.200
we really have four separate pools of eight nodes. And those eight nodes only talk to each other.

01:14:07.200 --> 01:14:12.480
But in total, there's like 32 nodes that are being processed. But there's sort of four separate

01:14:12.480 --> 01:14:18.160
pools of eight, you can look at it that way. The next note is that here in the case of language

01:14:18.160 --> 01:14:24.320
modeling, we have this specific structure of directed graph where the future tokens will

01:14:24.320 --> 01:14:28.960
not communicate to the past tokens. But this doesn't necessarily have to be the constraint

01:14:28.960 --> 01:14:34.800
in the general case. And in fact, in many cases, you may want to have all of the nodes talk to each

01:14:34.800 --> 01:14:39.200
other fully. So as an example, if you're doing sentiment analysis or something like that with

01:14:39.200 --> 01:14:43.920
a transformer, you might have a number of tokens, and you may want to have them all talk to each

01:14:43.920 --> 01:14:49.200
other fully. Because later you are predicting, for example, the sentiment of the sentence. And so

01:14:49.200 --> 01:14:54.560
it's okay for these nodes to talk to each other. And so in those cases, you will use an encoder

01:14:54.560 --> 01:15:00.640
block of self attention. And all it means that it's an encoder block is that you will delete

01:15:00.640 --> 01:15:05.360
this line of code, allowing all the nodes to completely talk to each other. What we're implementing

01:15:05.360 --> 01:15:11.520
here is sometimes called a decoder block. And it's called a decoder, because it is sort of like

01:15:11.520 --> 01:15:18.480
decoding language. And it's got this autoregressive format, where you have to mask with the triangle

01:15:18.480 --> 01:15:24.160
and matrix so that nodes from the future never talk to the past, because they would give away

01:15:24.160 --> 01:15:29.840
the answer. And so basically, in encoder blocks, you would delete this, allow all the nodes to talk.

01:15:29.840 --> 01:15:34.080
In decoder blocks, this will always be present so that you have this triangular structure.

01:15:34.720 --> 01:15:38.560
But both are allowed and attention doesn't care. Attention supports arbitrary connectivity

01:15:38.560 --> 01:15:43.760
between nodes. The next thing I wanted to comment on is you keep me you keep hearing me say attention,

01:15:43.760 --> 01:15:47.600
self attention, etc. There's actually also something called cross attention. What is the difference?

01:15:48.720 --> 01:15:56.480
So basically, the reason this attention is self attention is because the keys, queries,

01:15:56.480 --> 01:16:02.960
and the values are all coming from the same source from x. So the same source x produces keys,

01:16:02.960 --> 01:16:08.560
queries, and values. So these nodes are self attending. But in principle, attention is much

01:16:08.560 --> 01:16:14.400
more general than that. So for example, in encoder decoder transformers, you can have a case where

01:16:14.400 --> 01:16:19.280
the queries are produced from x. But the keys and the values come from a whole separate external

01:16:19.280 --> 01:16:24.800
source. And sometimes from encoder blocks that encode some context that we'd like to condition on.

01:16:25.360 --> 01:16:29.200
And so the keys and the values will actually come from a whole separate source. Those are

01:16:29.200 --> 01:16:34.000
nodes on the side. And here we're just producing queries. And we're reading off information from

01:16:34.000 --> 01:16:41.120
the side. So cross attention is used when there's a separate source of nodes, we'd like to pull

01:16:41.120 --> 01:16:45.920
information from into our notes. And it's self attention. If we just have nodes that would

01:16:45.920 --> 01:16:51.120
like to look at each other and talk to each other. So this attention here happens to be self attention.

01:16:52.640 --> 01:16:58.880
But in principle, attention is a lot more general. Okay, and the last note at this stage is

01:16:58.880 --> 01:17:03.520
if we come to the attention is all you need paper here, we've already implemented attention. So given

01:17:03.520 --> 01:17:09.680
query key and value, we've multiplied the query and the key, we've soft maxed it. And then we are

01:17:09.680 --> 01:17:14.000
aggregating the values. There's one more thing that we're missing here, which is the dividing by one

01:17:14.000 --> 01:17:19.440
over square root of the head size, the decay here is the head size. Why are they doing this one is

01:17:19.440 --> 01:17:25.680
this important. So they call it a scaled attention. And it's kind of like an important normalization

01:17:25.680 --> 01:17:32.000
to basically have the problem is if you have unit Gaussian inputs, so zero mean unit variance,

01:17:32.000 --> 01:17:37.200
k and q are unit Gaussian. And if you just do way naively, then you see that your way actually

01:17:37.200 --> 01:17:42.960
will be the variance will be on the order of head size, which in our case is 16. But if you

01:17:42.960 --> 01:17:47.040
multiply by one over head size square root, so this is square root, and this is one over,

01:17:48.240 --> 01:17:54.640
then the variance of way will be one, so we'll be preserved. Now why is this important? You'll

01:17:54.640 --> 01:18:02.240
notice that way here will feed into softmax. And so it's really important, especially at initialization,

01:18:02.240 --> 01:18:08.800
that way be fairly diffuse. So in our case here, we sort of locked out here and way

01:18:09.680 --> 01:18:16.400
had a fairly diffuse numbers here. So like this. Now the problem is that because of softmax,

01:18:16.400 --> 01:18:22.000
if weight takes on very positive and very negative numbers inside it, softmax will actually converge

01:18:22.000 --> 01:18:30.000
towards one hot vectors. And so I can illustrate that here. Say we are applying softmax to a

01:18:30.000 --> 01:18:34.480
tensor of values that are very close to zero, then we're gonna get a diffuse thing out of softmax.

01:18:35.440 --> 01:18:39.280
But the moment I take the exact same thing and I start sharpening it, making it bigger,

01:18:39.280 --> 01:18:44.160
by multiplying these numbers by eight, for example, you'll see that the softmax will start to sharpen.

01:18:44.160 --> 01:18:48.640
And in fact, it will sharpen towards the max. So it will sharpen towards whatever number here

01:18:48.640 --> 01:18:53.680
is the highest. And so basically, we don't want these values to be too extreme, especially at

01:18:53.680 --> 01:18:58.960
initialization. Otherwise, softmax will be way too peaky. And you're basically aggregating

01:19:00.160 --> 01:19:04.160
information from like a single node. Every node just aggregates information from a single other

01:19:04.160 --> 01:19:09.280
node. That's not what we want, especially at initialization. And so the scaling is used just

01:19:09.280 --> 01:19:14.320
to control the variance at initialization. Okay, so having said all that, let's now take our

01:19:14.320 --> 01:19:20.160
self attention knowledge and let's take it for a spin. So here in the code, I created this head

01:19:20.160 --> 01:19:25.920
module and implements a single head of self attention. So you give it a head size. And then

01:19:25.920 --> 01:19:30.720
here it creates the key query and the value linear layers. Typically people learn these biases and

01:19:30.720 --> 01:19:36.880
these. So those are the linear projections that we're going to apply to all of our nodes. Now here,

01:19:36.880 --> 01:19:42.160
I'm creating this trill variable. Trill is not a parameter of the module. So in sort of pytorch

01:19:42.160 --> 01:19:46.800
naming conventions, this is called a buffer. It's not a parameter. And you have to call it,

01:19:46.800 --> 01:19:50.640
you have to assign it to the module using a register buffer. So that creates the trill,

01:19:51.520 --> 01:19:56.320
the trying lower triangular matrix. And when we're given the input x, this should look very

01:19:56.320 --> 01:20:01.680
familiar. Now we calculate the keys, the queries, we can only calculate the attention scores inside

01:20:01.680 --> 01:20:07.840
way. We normalize it. So we're using scaled attention here. Then we made sure that sure

01:20:07.840 --> 01:20:13.440
doesn't communicate with the past. So this makes it a decoder block. And then softmax and then

01:20:13.440 --> 01:20:19.040
aggregate the value and output. Then here in the language model, I'm creating a head in the

01:20:19.680 --> 01:20:25.120
constructor. And I'm calling itself attention head. And the head size I'm going to keep as the same

01:20:25.120 --> 01:20:32.480
and embed just for now. And then here, once we've encoded the information with the token

01:20:32.480 --> 01:20:36.240
embeddings and the position embeddings, we're simply going to feed it into the self attention

01:20:36.240 --> 01:20:43.040
head. And then the output of that is going to go into the decoder language modeling head

01:20:43.040 --> 01:20:48.080
and create the logits. So this is sort of the simplest way to plug in a self attention component

01:20:48.880 --> 01:20:55.680
into our network right now. I had to make one more change, which is that here in the generate,

01:20:56.480 --> 01:21:02.000
we have to make sure that our IDX that we feed into the model, because now we're using positional

01:21:02.000 --> 01:21:08.720
embeddings, we can never have more than block size coming in. Because if IDX is more than block size,

01:21:08.720 --> 01:21:12.640
then our position embedding table is going to run out of scope because it only has embeddings for

01:21:12.640 --> 01:21:18.640
up to block size. And so therefore I added some code here to crop the context that we're going to

01:21:18.640 --> 01:21:26.400
feed into self. So that we never pass in more than block size elements. So those are the changes.

01:21:26.400 --> 01:21:30.880
And let's now train the network. Okay, so I also came up to the script here and I decreased the

01:21:30.880 --> 01:21:36.000
learning rate because the self attention can't tolerate very, very high learning rates. And then

01:21:36.000 --> 01:21:40.320
I also increased number of iterations because the learning rate is lower. And then I trained it and

01:21:40.320 --> 01:21:46.080
previously we were only able to get to up to 2.5. And now we are down to 2.4. So we definitely see

01:21:46.080 --> 01:21:52.880
a little bit of improvement from 2.5 to 2.4 roughly. But the text is still not amazing. So clearly the

01:21:52.880 --> 01:21:59.520
self attention head is doing some useful communication. But we still have a long way to go. Okay, so

01:21:59.520 --> 01:22:03.920
now we've implemented the scale.product attention. Now next up, and the attention is all you need

01:22:03.920 --> 01:22:08.160
to paper, there's something called multi head attention. And what is multi head attention?

01:22:08.720 --> 01:22:14.560
It's just applying multiple attentions in parallel and concatenating the results. So they have a

01:22:14.560 --> 01:22:20.240
little bit of diagram here. I don't know if this is super clear. It's really just multiple attentions

01:22:20.240 --> 01:22:27.120
in parallel. So let's implement that fairly straightforward. If we want a multi head attention,

01:22:27.120 --> 01:22:32.880
then we want multiple heads of self attention running in parallel. So in PyTorch, we can do this by

01:22:32.880 --> 01:22:40.560
simply creating multiple heads. So however many heads you want, and then what is the head size of each.

01:22:41.520 --> 01:22:47.200
And then we run all of them in parallel into a list and simply concatenate all of the

01:22:47.200 --> 01:22:53.520
outputs. And we're concatenating over the channel dimension. So the way this looks now is we don't

01:22:53.520 --> 01:23:00.480
have just a single attention that has a head size of 32. Because remember an embed is 32.

01:23:01.520 --> 01:23:08.080
Instead of having one communication channel, we now have four communication channels in parallel.

01:23:08.080 --> 01:23:14.720
And each one of these communication channels typically will be smaller correspondingly. So

01:23:14.720 --> 01:23:19.440
because we have four communication channels, we want eight dimensional self attention. And so

01:23:19.440 --> 01:23:23.840
from each communication channel, we're going to gather eight dimensional vectors. And then we have

01:23:23.840 --> 01:23:29.520
four of them. And that concatenates to give us 32, which is the original and embed. And so this is

01:23:29.520 --> 01:23:33.680
kind of similar to if you're familiar with convolutions, this is kind of like a group convolution.

01:23:34.320 --> 01:23:39.920
Because basically, instead of having one large convolution, we do convolution groups. And that's

01:23:39.920 --> 01:23:46.720
multi headed self attention. And so then here we just use SA heads, self attention heads instead.

01:23:46.720 --> 01:23:54.000
Now I actually ran it. And scrolling down. I ran the same thing. And then we now get this down to

01:23:54.000 --> 01:24:00.240
2.28 roughly. And the output is still the generation is still not amazing. But clearly the validation

01:24:00.240 --> 01:24:06.080
loss is improving because we were at 2.4 just now. And so it helps to have multiple communication

01:24:06.080 --> 01:24:11.280
channels because obviously, these tokens have a lot to talk about. They want to find the constants,

01:24:11.280 --> 01:24:15.600
the vowels, they want to find the vowels just from certain positions. They want to find

01:24:15.600 --> 01:24:19.040
they want to find any kinds of different things. And so it helps to create multiple independent

01:24:19.040 --> 01:24:24.720
channels of communication, gather lots of different types of data, and then decode the output.

01:24:24.720 --> 01:24:28.560
Now going back to the paper for a second, of course, I didn't explain this figure in full

01:24:28.560 --> 01:24:32.800
detail, but we are starting to see some components of what we've already implemented. We have the

01:24:32.800 --> 01:24:37.600
positional encodings, the token encodings that add, we have the masked multi headed attention

01:24:37.600 --> 01:24:42.960
implemented. Now, here's another multi headed attention, which is a cross attention to an

01:24:42.960 --> 01:24:47.200
encoder, which we haven't, we're not going to implement in this case. I'm going to come back

01:24:47.200 --> 01:24:52.240
to that later. But I want you to notice that there's a feet forward part here. And then this is

01:24:52.240 --> 01:24:56.800
grouped into a block that gets repeated to get in again. Now the feet forward part here is just a

01:24:56.800 --> 01:25:05.200
simple multi layer perceptron. So the multi headed. So here position wise feet forward networks is

01:25:05.200 --> 01:25:11.200
just a simple little MLP. So I want to start basically in a similar fashion, also adding computation

01:25:11.200 --> 01:25:17.840
into the network. And this computation is on the per node level. So I've already implemented it.

01:25:17.840 --> 01:25:22.720
And you can see the diff highlighted on the left here when I've added or changed things. Now before

01:25:22.720 --> 01:25:28.320
we had the self multi headed self attention that did the communication, but we went way too fast

01:25:28.320 --> 01:25:32.800
to calculate the logits. So the tokens looked at each other, but didn't really have a lot of time

01:25:32.800 --> 01:25:39.600
to think on what they found from the other tokens. And so what I've implemented here is a little

01:25:39.600 --> 01:25:45.360
feet forward single layer. And this little layer is just a linear filed by a relevant on linearity.

01:25:45.360 --> 01:25:54.480
And that's that's it. So it's just a little layer. And then I call it feet forward. And embed. And

01:25:54.480 --> 01:25:59.520
then this feet forward is just called sequentially right after the self attention. So we self attend,

01:25:59.520 --> 01:26:04.480
then we feed forward. And you'll notice that the feet forward here when it's applying linear, this

01:26:04.480 --> 01:26:09.760
is on the per token level, all the tokens do this independently. So the self attention is the

01:26:09.760 --> 01:26:14.000
communication. And then once they gathered all the data, now they need to think on that data

01:26:14.000 --> 01:26:20.080
individually. And so that's what feet forward is doing. And that's why I've added it here. Now when

01:26:20.080 --> 01:26:25.120
I train this, the validation laws actually continues to go down now to 2.24, which is down

01:26:25.120 --> 01:26:31.840
from 2.28. The output still looked kind of terrible. But at least we've improved the situation. And so

01:26:31.840 --> 01:26:39.280
as a preview, we're going to now start to interspers the communication with the computation. And that's

01:26:39.280 --> 01:26:45.280
also what the transformer does when it has blocks that communicate and then compute, and it groups

01:26:45.280 --> 01:26:51.440
them and replicates them. Okay, so let me show you what we'd like to do. We'd like to do something

01:26:51.440 --> 01:26:56.320
like this, we have a block. And this block is basically this part here, except for the cross

01:26:56.320 --> 01:27:02.560
attention. Now the block basically intersperses communication and the computation, the computation,

01:27:02.560 --> 01:27:07.360
the communication is done using multi headed self attention. And then the computation is done using

01:27:07.360 --> 01:27:14.080
a feet forward network on all the tokens independently. Now, what I've added here also is

01:27:14.080 --> 01:27:19.840
you'll notice, this takes the number of embeddings in the embedding dimension and number of heads that

01:27:19.840 --> 01:27:24.640
we would like, which is kind of like group sizing group convolution. And I'm saying that number of

01:27:24.640 --> 01:27:30.560
heads we like is four. And so because this is 32, we calculate that because this 32, the number of

01:27:30.560 --> 01:27:36.800
heads should be four, the head size should be eight, so that everything sort of works out channel

01:27:36.800 --> 01:27:44.400
wise. So this is how the transformer structures sort of the sizes typically. So the head size will

01:27:44.400 --> 01:27:49.040
become eight, and then this is how we want to interspers them. And then here, I'm trying to create

01:27:49.040 --> 01:27:54.720
blocks, which is just a sequential application of block block block. So that we're interspersing,

01:27:54.720 --> 01:28:00.800
communication, feet forward many, many times. And then finally, we decode. Now, actually try to run

01:28:00.800 --> 01:28:06.720
this. And the problem is this doesn't actually give a very good answer. And very good result. And

01:28:06.720 --> 01:28:11.440
the reason for that is we're starting to actually get like a pretty deep neural net. And deep neural

01:28:11.440 --> 01:28:15.520
nets suffer from optimization issues. And I think that's what we're kind of like slightly starting

01:28:15.520 --> 01:28:21.440
to run into. So we need one more idea that we can borrow from the transformer paper to resolve

01:28:21.440 --> 01:28:26.240
those difficulties. Now there are two optimizations that dramatically help with the depth of these

01:28:26.240 --> 01:28:30.960
networks and make sure that the networks remain optimizable. Let's talk about the first one.

01:28:31.520 --> 01:28:37.280
The first one in this diagram is you see this arrow here. And then this arrow and this arrow,

01:28:37.280 --> 01:28:42.160
those are skip connections, or sometimes called residual connections. They come from this paper,

01:28:42.160 --> 01:28:48.800
the Procedural Learning Form and Recognition from about 2015, that introduced the concept. Now,

01:28:49.600 --> 01:28:54.400
these are basically what it means is you transform the data, but then you have a skip connection

01:28:54.400 --> 01:29:00.960
with addition from the previous features. Now the way I like to visualize it, that I prefer,

01:29:01.520 --> 01:29:07.520
is the following. Here the computation happens from the top to bottom. And basically, you have this

01:29:07.520 --> 01:29:14.800
residual pathway. And you are free to fork off from the residual pathway, perform some computation,

01:29:14.800 --> 01:29:21.840
and then project back to the residual pathway via addition. And so you go from the inputs to the

01:29:21.840 --> 01:29:27.120
targets only via plus and plus and plus. And the reason this is useful is because during bad

01:29:27.120 --> 01:29:33.120
propagation, remember from our micrograd video earlier, addition distributes gradients equally

01:29:33.120 --> 01:29:40.320
to both of its branches that that fed as the input. And so the supervision or the gradients from the

01:29:40.320 --> 01:29:48.160
loss basically hop through every addition node all the way to the input, and then also fork off

01:29:48.160 --> 01:29:54.080
into the residual blocks. But basically, you have this gradient superhighway that goes directly

01:29:54.080 --> 01:29:59.360
from the supervision all the way to the input unimpeded. And then these original blocks are

01:29:59.360 --> 01:30:03.280
usually initialized in the beginning. So they contribute very, very little, if anything, to the

01:30:03.280 --> 01:30:08.960
residual pathway. They are initialized that way. So in the beginning, they are almost kind of like

01:30:08.960 --> 01:30:15.520
not there. But then during the optimization, they come online over time. And they start to contribute.

01:30:16.160 --> 01:30:21.120
But at least at the initialization, you can go from directly supervision to the input gradient

01:30:21.120 --> 01:30:27.280
is unimpeded and just flows. And then the blocks over time kick in. And so that dramatically helps

01:30:27.280 --> 01:30:31.920
with the optimization. So let's implement this. So coming back to our block here, basically what we

01:30:31.920 --> 01:30:39.440
want to do is we want to do x equals x plus self attention and x equals x plus self that feed forward.

01:30:40.560 --> 01:30:46.480
So this is x. And then we fork off and do some communication and come back. And we fork off and

01:30:46.480 --> 01:30:52.160
we do some computation and come back. So those are residual connections. And then swinging back up

01:30:52.160 --> 01:31:00.400
here, we also have to introduce this projection. So nn.linear. And this is going to be from

01:31:01.760 --> 01:31:06.560
after we concatenate this, this is the size and embed. So this is the output of the self tension

01:31:06.560 --> 01:31:14.400
itself. But then we actually want the to apply the projection. And that's the result. So the

01:31:14.400 --> 01:31:19.760
projection is just a linear transformation of the outcome of this layer. So that's the projection

01:31:19.760 --> 01:31:24.240
back into the residual pathway. And then here in a feed forward, it's going to be the same thing.

01:31:24.800 --> 01:31:30.320
I could have a self that projection here as well. But let me just simplify it. And let me

01:31:32.000 --> 01:31:36.720
couple it inside the same sequential container. And so this is the projection layer going back

01:31:36.720 --> 01:31:44.320
into the residual pathway. And so that's, well, that's it. So now we can train this. So I implemented

01:31:44.320 --> 01:31:50.480
one more small change. When you look into the paper again, you see that the dimensionality of input

01:31:50.480 --> 01:31:55.200
and output is 512 for them. And they're saying that the inner layer here in the feed forward has

01:31:55.200 --> 01:32:01.440
dimensionality of 2048. So there's a multiplier of four. And so the inner layer of the feed forward

01:32:01.440 --> 01:32:06.480
network should be multiplied by four in terms of channel sizes. So I came here and I multiplied

01:32:06.480 --> 01:32:11.920
four times embed here for the feed forward. And then from four times and embed coming back down

01:32:11.920 --> 01:32:16.800
to an embed when we go back to the project to the projection. So adding a bit of computation here

01:32:16.800 --> 01:32:21.840
and growing that layer that is in the residual block on the side of the residual pathway.

01:32:22.960 --> 01:32:28.080
And then I trained this and we actually get down all the way to 2.08 validation loss.

01:32:28.080 --> 01:32:31.760
And we also see that the network is starting to get big enough that our train loss is getting

01:32:31.760 --> 01:32:37.680
ahead of validation loss. So we started to see like a little bit of overfitting. And our

01:32:37.680 --> 01:32:44.800
generations here are still not amazing. But at least you see that we can see like is here this now

01:32:44.800 --> 01:32:50.800
grief sync like this starts to almost look like English. So yeah, we're starting to really get

01:32:50.800 --> 01:32:55.680
there. Okay. And the second innovation that is very helpful for optimizing very deep neural networks

01:32:55.680 --> 01:33:00.080
is right here. So we have this addition now that's the residual part. But this norm is

01:33:00.080 --> 01:33:04.960
referring to something called layer norm. So layer norm is implemented in PyTorch. It's a paper that

01:33:04.960 --> 01:33:13.600
came out a while back here. And layer norm is very, very similar to bash norm. So remember back to

01:33:14.400 --> 01:33:20.000
our Make More series part three, we implemented bash normalization. And bash normalization basically

01:33:20.000 --> 01:33:30.080
just made sure that across the bash dimension, any individual neuron had unit Gaussian distribution.

01:33:30.080 --> 01:33:36.640
So it was zero mean and unit standard deviation, one standard deviation output. So what I did here

01:33:36.640 --> 01:33:42.080
is I'm copy pasting the bash norm one d that we developed in our Make More series. And see here,

01:33:42.080 --> 01:33:48.000
we can initialize, for example, this module, and we can have a batch of 32 100 dimensional vectors

01:33:48.000 --> 01:33:54.560
feeding through the bash norm layer. So what this does is it guarantees that when we look at just the

01:33:54.560 --> 01:34:01.280
zero column, it's a zero mean one standard deviation. So it's normalizing every single

01:34:01.280 --> 01:34:07.920
column of this input. Now the rows are not going to be normalized by default, because we're just

01:34:07.920 --> 01:34:14.960
normalizing columns. So let's not implement layer norm. It's very complicated. Look, we come here,

01:34:14.960 --> 01:34:20.800
we change this from zero to one. So we don't normalize the columns, we normalize the rows.

01:34:20.800 --> 01:34:27.440
And now we've implemented layer norm. So now the columns are not going to be normalized.

01:34:28.800 --> 01:34:34.320
But the rows are going to be normalized for every individual example, it's 100 dimensional vector is

01:34:34.320 --> 01:34:41.520
normalized in this way. And because our computation now does not span across examples, we can delete

01:34:41.520 --> 01:34:48.480
all of this buffer stuff, because we can always apply this operation, and don't need to maintain

01:34:48.480 --> 01:34:56.640
any running buffers. So we don't need the buffers. We don't, there's no distinction between training

01:34:56.640 --> 01:35:04.000
and test time. And we don't need these running buffers, we do keep gamma and beta, we don't need

01:35:04.000 --> 01:35:12.000
the momentum, we don't care if it's training or not. And this is now a layer norm. And it normalizes

01:35:12.000 --> 01:35:20.320
the rows instead of the columns. And this here is identical to basically this here. So let's now

01:35:20.320 --> 01:35:24.560
implement layer norm in our transformer. Before I incorporate the layer norm, I just wanted to note

01:35:24.560 --> 01:35:28.960
that, as I said, very few details about the transformer have changed in the last five years.

01:35:28.960 --> 01:35:33.360
But this is actually something that slightly departs from the original paper. You see that the

01:35:33.360 --> 01:35:41.440
add and norm is applied after the transformation. But in now it is a bit more basically common to

01:35:41.440 --> 01:35:46.000
apply the layer norm before the transformation. So there's a reshuffling of the layer norms.

01:35:46.800 --> 01:35:50.480
So this is called the pre norm formulation, and that the one that we're going to implement as well.

01:35:50.480 --> 01:35:55.680
So select deviation from the original paper. Basically, we need to layer norms layer norm one

01:35:55.680 --> 01:36:03.360
is an end dot layer norm. And we tell it how many words the embedding dimension. And we need the

01:36:03.360 --> 01:36:10.000
second layer norm. And then here, the layer norms are applied immediately on x. So self dot

01:36:10.000 --> 01:36:16.480
layer norm one applied on x and self dot layer norm two applied on x before it goes into self

01:36:16.480 --> 01:36:23.440
attention and feed forward. And the size of the layer norm here is an embed so 32. So when the

01:36:23.440 --> 01:36:31.840
layer norm is normalizing our features, it is the normalization here happens, the mean and the

01:36:31.840 --> 01:36:37.600
variance are taken over 32 numbers. So the batch and the time act as batch dimensions, both of them.

01:36:37.600 --> 01:36:43.920
So this is kind of like a per token transformation that just normalizes the features and makes them

01:36:43.920 --> 01:36:50.720
a unit mean unit Gaussian at initialization. But of course, because these layer norms inside it

01:36:51.280 --> 01:36:58.400
have these gamma and beta trainable parameters, the layer normal eventually create outputs that

01:36:58.400 --> 01:37:03.840
might not be unit Gaussian, but the optimization will determine that. So for now, this is the

01:37:03.840 --> 01:37:08.800
this is incorporating the layer norms and let's train them up. Okay, so I let it run. And we see

01:37:08.800 --> 01:37:14.320
that we get down to 2.06, which is better than the previous 2.08. So a slight improvement by adding

01:37:14.320 --> 01:37:19.120
the layer norms. And I'd expect that they help even more if we had bigger and deeper network.

01:37:19.840 --> 01:37:24.080
One more thing I forgot to add is that there should be a layer norm here also typically,

01:37:24.640 --> 01:37:30.240
as at the end of the transformer and right before the final linear layer that decodes into

01:37:30.240 --> 01:37:36.160
vocabulary. So I added that as well. So at this stage, we actually have a pretty complete transformer

01:37:36.160 --> 01:37:41.200
coming to the original paper. And it's a decoder only transformer. I'll talk about that in a second.

01:37:41.760 --> 01:37:46.160
But at this stage, the major pieces are in place. So we can try to scale this up and see how well

01:37:46.160 --> 01:37:50.960
we can push this number. Now, in order to scale out the model, I had to perform some cosmetic

01:37:50.960 --> 01:37:56.160
changes here to make it nicer. So I introduced this variable called n layer, which just specifies

01:37:56.160 --> 01:38:01.200
how many layers of the blocks we're going to have. I create a bunch of blocks and we have a new

01:38:01.200 --> 01:38:07.040
variable number of heads as well. I pulled out the layer norm here. And so this is identical.

01:38:07.680 --> 01:38:13.840
Now, one thing that I did briefly change is I added dropout. So dropout is something that you can add

01:38:13.840 --> 01:38:19.120
right before the residual connection back, right before the connection back into the residual

01:38:19.120 --> 01:38:24.400
pathway. So we can drop out that as the last layer here. We can drop out

01:38:24.400 --> 01:38:30.400
here at the end of the multiheaded decision as well. And we can also drop out here when we

01:38:30.400 --> 01:38:37.040
calculate the basically affinities and after the softmax, we can drop out some of those so we can

01:38:37.040 --> 01:38:43.440
randomly prevent some of the notes from communicating. And so dropout comes from this paper from

01:38:43.440 --> 01:38:52.560
2014 or so. And basically it takes your neural net and it randomly every forward backward pass

01:38:52.560 --> 01:39:00.160
shuts off some subset of neurons. So randomly drops them to zero and trains without them.

01:39:00.560 --> 01:39:05.360
And what this does effectively is because the mask of what's being dropped out is changed every

01:39:05.360 --> 01:39:11.760
single forward backward pass, it ends up kind of training an ensemble of subnetworks. And then at

01:39:11.760 --> 01:39:16.160
test time, everything is fully enabled and kind of all of those subnetworks are merged into a

01:39:16.160 --> 01:39:20.720
single ensemble if you can, if you want to think about it that way. So I would read the paper to

01:39:20.720 --> 01:39:25.360
get the full detail. For now, we're just going to stay on the level of this is a regularization

01:39:25.360 --> 01:39:30.000
technique. And I added it because I'm about to scale up the model quite a bit. And I was concerned

01:39:30.000 --> 01:39:35.840
about overfitting. So now when we scroll up to the top, we'll see that I changed a number of

01:39:35.840 --> 01:39:41.040
hyper parameters here about our neural net. So I made the batch size be much larger. Now 64.

01:39:41.760 --> 01:39:46.720
I changed the block size to be 256. So previously it was just eight, eight characters of context.

01:39:46.720 --> 01:39:54.560
Now it is 256 characters of context to predict the 257th. I brought down the learning rate a little

01:39:54.560 --> 01:39:59.280
bit because the neural net is now much bigger. So I brought down the learning rate. The embedding

01:39:59.280 --> 01:40:06.720
dimension is now 384. And there are six heads. So 384 divide six means that every head is 64

01:40:06.720 --> 01:40:13.120
dimensional as it as a standard. And then there are going to be six layers of that. And the dropout

01:40:13.120 --> 01:40:20.000
will be a point two. So every forward backward pass 20% of all of these intermediate calculations

01:40:20.000 --> 01:40:26.560
are disabled and dropped to zero. And then I already trained this and I ran it. So drumroll,

01:40:26.560 --> 01:40:34.400
how does it perform? So let me just scroll up here. We get a validation loss of 1.48,

01:40:34.400 --> 01:40:39.040
which is actually quite a bit of an improvement on what we had before, which I think was 2.07.

01:40:39.040 --> 01:40:45.040
So we went from 2.07 all the way down to 1.48 just by scaling up this neural net with the code

01:40:45.040 --> 01:40:49.840
that we have. And this of course ran for a lot longer. This maybe trained for, I want to say,

01:40:49.840 --> 01:40:55.440
about 15 minutes on my A100 GPU. So that's a pretty good GPU. And if you don't have a GPU,

01:40:55.440 --> 01:41:00.880
you're not going to be able to reproduce this. On a CPU, this would be, I would not run this on a

01:41:00.880 --> 01:41:05.760
CPU or MacBook or something like that. You'll have to break down the number of layers and the

01:41:05.760 --> 01:41:11.280
embedding dimension and so on. But in about 15 minutes, we can get this kind of a result. And

01:41:12.400 --> 01:41:17.840
I'm printing some of the Shakespeare here, but what I did also is I printed 10,000 characters,

01:41:17.840 --> 01:41:21.840
so a lot more, and I wrote them to a file. And so here we see some of the outputs.

01:41:24.160 --> 01:41:29.920
So it's a lot more recognizable as the input text file. So the input text file just for reference

01:41:29.920 --> 01:41:37.600
looked like this. So there's always someone speaking in this matter. And our predictions now

01:41:37.600 --> 01:41:42.960
take on that form. Except of course, they're non-surgical when you actually read them. So

01:41:44.480 --> 01:41:50.400
it is, every crimpty B house, oh, those prepation, we give heed.

01:41:50.400 --> 01:42:03.920
Oh, ho, sent me you mighty lord. Anyway, so you can read through this. It's non-surgical, of course,

01:42:03.920 --> 01:42:08.960
but this is just a transformer trained on the character level for one million characters

01:42:08.960 --> 01:42:13.760
that come from Shakespeare. So they're sort of like blabbers on in Shakespeare-like manner,

01:42:13.760 --> 01:42:19.600
but it doesn't, of course, make sense at this scale. But I think still a pretty good demonstration

01:42:19.600 --> 01:42:27.840
of what's possible. So now I think that kind of like concludes the programming section of this

01:42:27.840 --> 01:42:34.880
video. We basically kind of did a pretty good job in implementing this transformer, but the picture

01:42:34.880 --> 01:42:39.840
doesn't exactly match up to what we've done. So what's going on with all these digital parts here?

01:42:39.840 --> 01:42:44.480
So let me finish explaining this architecture and why it looks so funky. Basically, what's

01:42:44.480 --> 01:42:50.720
happening here is what we implemented here is a decoder only transformer. So there's no component

01:42:50.720 --> 01:42:56.800
here. This part is called the encoder, and there's no cross attention block here. Our block only has

01:42:56.800 --> 01:43:02.800
a self attention and the feed forward. So it is missing this third in between piece here.

01:43:02.800 --> 01:43:07.200
This piece does cross attention. So we don't have it and we don't have the encoder. We just have the

01:43:07.200 --> 01:43:13.360
decoder. And the reason we have a decoder only is because we are just generating text and it's

01:43:13.360 --> 01:43:17.440
unconditioned on anything. We're just we're just blabbering on according to a given data set.

01:43:18.240 --> 01:43:24.320
What makes it a decoder is that we are using the triangular mask in our transformer. So it has this

01:43:24.320 --> 01:43:29.520
autoregressive property where we can just go and sample from it. So the fact that it's using the

01:43:29.520 --> 01:43:34.960
triangulate triangular mask to mask out the attention makes it a decoder. And it can be used

01:43:34.960 --> 01:43:40.800
for language modeling. Now, the reason that the original paper had an encoder decoder architecture

01:43:40.800 --> 01:43:46.080
is because it is a machine translation paper. So it is concerned with a different setting in particular.

01:43:46.800 --> 01:43:53.760
It expects some tokens that encode, say for example, French. And then it is expected to decode

01:43:53.760 --> 01:44:00.640
the translation in English. So you typically these here are special tokens. So you are expected to

01:44:00.640 --> 01:44:05.600
read in this and condition on it. And then you start off the generation with a special token

01:44:05.600 --> 01:44:11.600
called start. So this is a special new token that you introduce and always place in the beginning.

01:44:12.240 --> 01:44:18.480
And then the network is expected to output neural networks are awesome. And then a special end token

01:44:18.480 --> 01:44:25.440
to finish the generation. So this part here will be decoded exactly as we have done it.

01:44:25.440 --> 01:44:31.200
Neural networks are awesome will be identical to what we did. But unlike what we did, they want to

01:44:31.200 --> 01:44:36.880
condition the generation on some additional information. And in that case, this additional

01:44:36.880 --> 01:44:41.440
information is the French sentence that they should be translating. So what they do now

01:44:42.400 --> 01:44:49.040
is they bring the encoder. Now the encoder reads this part here. So we're only going to take the

01:44:49.040 --> 01:44:54.640
part of French. And we're going to create tokens from it exactly as we've seen in our video. And

01:44:54.640 --> 01:44:59.920
we're going to put a transformer on it. But there's going to be no triangular mask. And so all the

01:44:59.920 --> 01:45:04.800
tokens are allowed to talk to each other as much as they want. And they're just encoding whatever's

01:45:04.800 --> 01:45:12.240
the content of this French sentence. Once they've encoded it, they basically come out in the top

01:45:12.240 --> 01:45:18.960
here. And then what happens here is in our decoder, which does the language modeling, there's an

01:45:18.960 --> 01:45:24.640
additional connection here to the outputs of the encoder. And that is brought in through

01:45:24.640 --> 01:45:31.200
cross attention. So the queries are still generated from X. But now the keys and the values are coming

01:45:31.200 --> 01:45:36.800
from the side, the keys and the values are coming from the top generated by the nodes that came

01:45:36.800 --> 01:45:42.560
outside of the decode the encoder. And those tops, the keys and the values there, the top of it,

01:45:43.680 --> 01:45:48.800
feed in on a side into every single block of the decoder. And so that's why there's an additional

01:45:48.800 --> 01:45:54.880
cross attention. And really what it's doing is it's conditioning the decoding, not just on the past

01:45:54.880 --> 01:46:04.000
of this current decoding, but also on having seen the full fully encoded French prompt sort of.

01:46:04.880 --> 01:46:08.160
And so it's an encoder decoder model, which is why we have those two transformers,

01:46:08.160 --> 01:46:13.280
an additional block, and so on. So we did not do this because we have no, we have nothing to encode,

01:46:13.280 --> 01:46:17.200
there's no conditioning, we just have a text file, and we just want to imitate it. And that's why

01:46:17.200 --> 01:46:24.160
we are using a decoder only transformer, exactly as done in GPT. Okay, so now I wanted to do a very

01:46:24.160 --> 01:46:30.240
brief walkthrough of nano GPT, which you can find in my GitHub. And then GPT is basically two files

01:46:30.240 --> 01:46:36.240
of interest. There's train.py and model.py. train.py is all the boilerplate code for training the

01:46:36.240 --> 01:46:42.480
network. It is basically all the stuff that we had here is the training loop. It's just that it's

01:46:42.480 --> 01:46:46.720
a lot more complicated because we're saving and loading checkpoints and pre trained weights. And

01:46:46.720 --> 01:46:51.520
we are decaying the learning rate and compiling the model and using distributed training across

01:46:51.520 --> 01:46:57.760
multiple nodes or GPUs. So the train.py gets a little bit more hairy, complicated. There's more

01:46:57.760 --> 01:47:04.480
options, etc. But the model.py should look very, very similar to what we've done here. In fact,

01:47:04.480 --> 01:47:11.520
the model is almost identical. So first, here we have the causal self attention block. And all of

01:47:11.520 --> 01:47:16.320
this should look very, very recognizable to you. We're producing query skis values. We're doing

01:47:16.320 --> 01:47:22.480
dot products. We're masking applying softmax optionally dropping out. And here we are pooling

01:47:22.480 --> 01:47:30.480
the way the values. What is different here is that in our code, I have separated out the multi headed

01:47:30.480 --> 01:47:36.080
attention into just a single individual head. And then here I have multiple heads and I explicitly

01:47:36.080 --> 01:47:42.080
concatenate them. Whereas here, all of it is implemented in a batched manner inside a single

01:47:42.080 --> 01:47:47.360
causal self attention. And so we don't just have a B and a T and a C dimension, we also end up with

01:47:47.360 --> 01:47:52.640
a fourth dimension, which is the heads. And so it just gets a lot more sort of hairy because we have

01:47:52.640 --> 01:47:58.960
four dimensional array, tensors now, but it is equivalent mathematically. So the exact same

01:47:58.960 --> 01:48:03.440
thing is happening is what we have. It's just a bit more efficient because all the heads are

01:48:03.440 --> 01:48:08.640
not treated as a batch dimension as well. Then we have the multi layer perceptron, it's using the

01:48:08.640 --> 01:48:14.160
galon non linearity, which is defined here, except instead of really, and this is done just because

01:48:14.160 --> 01:48:19.360
openly I used it and I want to be able to load their checkpoints. The blocks of the transformer

01:48:19.360 --> 01:48:24.640
are identical to communicate and the compute phase as we saw. And then the GPT will be identical,

01:48:24.640 --> 01:48:30.560
we have the positioning coatings, token encodings, the blocks, the layer norm at the end, the final

01:48:30.560 --> 01:48:36.080
linear layer. And this should look all very recognizable. And there's a bit more here,

01:48:36.080 --> 01:48:40.080
because I'm loading checkpoints and stuff like that. I'm separating out the parameters into

01:48:40.080 --> 01:48:45.120
those that should be weight decayed and those that shouldn't. But the generate function should

01:48:45.120 --> 01:48:49.600
also be very, very similar. So a few details are different, but you should definitely be able to

01:48:49.600 --> 01:48:55.040
look at this file and be able to understand a lot of the pieces now. So let's now bring things back

01:48:55.040 --> 01:49:00.320
to chat GPT. What would it look like if we wanted to train chat GPT ourselves? And how does it relate

01:49:00.320 --> 01:49:05.680
to what we learned today? Well, to train in chat GPT, there are roughly two stages. First is the

01:49:05.680 --> 01:49:11.840
pre-training stage and then the fine-tuning stage. In the pre-training stage, we are training on a

01:49:11.840 --> 01:49:18.080
large chunk of internet and just trying to get a first decoder only transformer to babble text.

01:49:18.640 --> 01:49:24.080
So it's very, very similar to what we've done ourselves, except we've done like a tiny little

01:49:24.080 --> 01:49:32.000
baby pre-training step. And so in our case, this is how you print a number of parameters. I printed

01:49:32.000 --> 01:49:37.120
it and it's about 10 million. So this transformer that I created here to create little Shakespeare

01:49:38.640 --> 01:49:44.800
transformer was about 10 million parameters. Our dataset is roughly one million characters,

01:49:44.800 --> 01:49:49.440
so roughly one million tokens. But you have to remember that OpenAI uses different vocabulary.

01:49:49.440 --> 01:49:54.960
They're not on the character level. They use these subword chunks of words. And so they have a

01:49:54.960 --> 01:50:02.320
vocabulary of 50,000 roughly elements. And so their sequences are a bit more condensed. So our dataset,

01:50:02.320 --> 01:50:08.160
the Shakespeare dataset, would be probably around 300,000 tokens in the OpenAI vocabulary, roughly.

01:50:09.120 --> 01:50:14.800
So we trained about 10 million parameter model and roughly 300,000 tokens. Now, when you go to the

01:50:14.800 --> 01:50:22.800
GPT3 paper and you look at the transformers that they trained, they trained a number of transformers

01:50:22.800 --> 01:50:29.280
of different sizes. But the biggest transformer here has 175 billion parameters. So ours is again

01:50:29.280 --> 01:50:35.520
10 million. They used this number of layers in the transformer. This is the N embed. This is the

01:50:35.520 --> 01:50:42.400
number of heads. And this is the head size. And then this is the batch size. So ours was 65.

01:50:44.000 --> 01:50:49.280
And the learning rate is similar. Now, when they train this transformer, they trained on 300 billion

01:50:49.280 --> 01:50:56.960
tokens. So again, remember, ours is about 300,000. So this is about a million fold increase. And this

01:50:56.960 --> 01:51:01.360
number would not be even that large by today's standards. You'd be going up one trillion and

01:51:01.360 --> 01:51:09.840
above. So they are training a significantly larger model on a good chunk of the internet.

01:51:09.840 --> 01:51:14.400
And that is the pre-training stage. But otherwise, these hyperparameters should be fairly recognizable

01:51:14.400 --> 01:51:19.280
to you. And the architecture is actually nearly identical to what we implemented ourselves.

01:51:19.280 --> 01:51:23.600
But of course, it's a massive infrastructure challenge to train this. You're talking about

01:51:23.600 --> 01:51:29.680
typically thousands of GPUs having to talk to each other to train models of this size.

01:51:29.680 --> 01:51:33.440
So that's just the pre-training stage. Now, after you complete the pre-training stage,

01:51:34.320 --> 01:51:39.200
you don't get something that responds to your questions with answers and it's not helpful and

01:51:39.200 --> 01:51:46.880
et cetera. You get a document completer. But it doesn't babble Shakespeare, it babbles internet.

01:51:46.880 --> 01:51:51.040
It will create arbitrary news articles and documents and it will try to complete documents

01:51:51.040 --> 01:51:54.720
because that's what it's trained for. It's trying to complete the sequence. So when you give it a

01:51:54.720 --> 01:52:00.160
question, it would potentially just give you more questions. It would follow with more questions.

01:52:00.160 --> 01:52:05.840
It will do whatever it looks like some close document would do in the training data on the

01:52:05.840 --> 01:52:10.240
internet. And so who knows, you're getting kind of like undefined behavior. It might basically

01:52:10.240 --> 01:52:15.120
answer with two questions with other questions. It might ignore your question. It might just try

01:52:15.120 --> 01:52:21.040
to complete some news article. It's totally undermined, as we say. So the second fine-tuning

01:52:21.040 --> 01:52:27.760
stage is to actually align it to be an assistant. And this is the second stage. And so this chat

01:52:27.760 --> 01:52:32.960
gpt block post from opening I talks a little bit about how this stage is achieved. We basically

01:52:32.960 --> 01:52:39.920
there's roughly three steps to this stage. So what they do here is they start to collect

01:52:39.920 --> 01:52:44.960
training data that looks specifically like what an assistant would do. So they are documents that

01:52:44.960 --> 01:52:49.600
have the format where the question is on top and then an answer is below. And they have a large

01:52:49.600 --> 01:52:53.840
number of these, but probably not on the order of the internet. This is probably on the order of

01:52:53.840 --> 01:53:02.720
maybe thousands of examples. And so they then fine-tune the model to basically only focus on

01:53:02.720 --> 01:53:06.960
documents that look like that. And so you're starting to slowly align it. So it's going to

01:53:06.960 --> 01:53:11.920
expect a question at the top and it's going to expect to complete the answer. And these very,

01:53:11.920 --> 01:53:16.960
very large models are very sample efficient during their fine-tuning. So this actually somehow works.

01:53:17.680 --> 01:53:22.320
But that's just step one. That's just fine-tuning. So then they actually have more steps where,

01:53:22.320 --> 01:53:26.880
okay, the second step is you let the model respond and then different raters look at the

01:53:26.880 --> 01:53:31.280
different responses and rank them for their preference as to which one is better than the other.

01:53:31.840 --> 01:53:36.960
They use that to train a reward model. So they can predict basically using a different network,

01:53:36.960 --> 01:53:44.400
how much of any candidate response would be desirable. And then once they have a reward model,

01:53:44.400 --> 01:53:49.760
they run PPO, which is a form of policy gradient reinforcement learning optimizer,

01:53:49.760 --> 01:53:59.040
to fine-tune this sampling policy so that the answers that the chatGPT now generates are expected

01:53:59.040 --> 01:54:05.440
to score a high reward according to the reward model. And so basically there's a whole aligning

01:54:05.440 --> 01:54:11.440
stage here or fine-tuning stage. It's got multiple steps in between there as well. And it takes the

01:54:11.440 --> 01:54:17.440
model from being a document completer to a question answerer. And that's like a whole separate

01:54:17.440 --> 01:54:24.000
stage. A lot of this data is not available publicly. It is internal to OpenAI and it's much harder to

01:54:24.000 --> 01:54:30.480
replicate this stage. And so that's roughly what would give you a chatGPT. And nanoGPT

01:54:30.480 --> 01:54:34.240
focuses on the pre-training stage. Okay, and that's everything that I wanted to cover today.

01:54:34.960 --> 01:54:42.080
So we trained, to summarize, a decoder-only transformer following this famous paper,

01:54:42.080 --> 01:54:48.000
Attention is All You Need from 2017. And so that's basically a GPT. We trained it on

01:54:48.880 --> 01:54:56.480
Tiny Shakespeare and got sensible results. All of the training code is roughly 200 lines of code.

01:54:57.120 --> 01:55:04.480
I will be releasing this code base. So also it comes with all the Git log commits along the way

01:55:04.480 --> 01:55:10.800
as we built it up. In addition to this code, I'm going to release the notebook, of course,

01:55:10.800 --> 01:55:16.960
the Google collab. And I hope that gave you a sense for how you can train these models like,

01:55:16.960 --> 01:55:22.320
say, GPT3. That will be architecturally basically identical to what we have. But they are somewhere

01:55:22.320 --> 01:55:29.440
between 10,000 and 1 million times bigger, depending on how you count. And so that's all I have for now.

01:55:30.080 --> 01:55:34.400
We did not talk about any of the fine-tuning stages that would typically go on top of this.

01:55:34.400 --> 01:55:38.080
So if you're interested in something that's not just language modeling, but you actually want to,

01:55:38.080 --> 01:55:43.600
you know, say perform tasks, or you want them to be aligned in a specific way, or you want

01:55:44.480 --> 01:55:47.840
to detect sentiment or anything like that, basically, any time you don't want something

01:55:47.840 --> 01:55:51.920
that's just a document completer, you have to complete further stages of fine-tuning,

01:55:51.920 --> 01:55:57.120
which we did not cover. And that could be simple, supervised fine-tuning, or it can be something

01:55:57.120 --> 01:56:01.600
more fancy, like we see in chat GPT, where you actually train a reward model and then do rounds

01:56:01.600 --> 01:56:06.640
of PPO to align it with respect to the reward model. So there's a lot more that can be done on top

01:56:06.640 --> 01:56:12.320
of it. I think for now we're starting to get to about two hours mark. So I'm going to kind of

01:56:12.320 --> 01:56:37.680
finish here. I hope you enjoyed the lecture. And yeah, go forth and transform. See you later.

